{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 460,
   "id": "6b601afa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-20T12:41:34.801207Z",
     "iopub.status.busy": "2022-02-20T12:41:34.799731Z",
     "iopub.status.idle": "2022-02-20T12:41:36.957435Z",
     "shell.execute_reply": "2022-02-20T12:41:36.958645Z",
     "shell.execute_reply.started": "2022-02-20T12:38:48.333592Z"
    },
    "papermill": {
     "duration": 2.184476,
     "end_time": "2022-02-20T12:41:36.958991",
     "exception": false,
     "start_time": "2022-02-20T12:41:34.774515",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506ec941",
   "metadata": {
    "papermill": {
     "duration": 0.03617,
     "end_time": "2022-02-20T12:41:37.032468",
     "exception": false,
     "start_time": "2022-02-20T12:41:36.996298",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "id": "c088d4d6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-20T12:41:37.110359Z",
     "iopub.status.busy": "2022-02-20T12:41:37.108889Z",
     "iopub.status.idle": "2022-02-20T12:41:37.179781Z",
     "shell.execute_reply": "2022-02-20T12:41:37.180176Z",
     "shell.execute_reply.started": "2022-02-20T12:38:48.342682Z"
    },
    "papermill": {
     "duration": 0.112529,
     "end_time": "2022-02-20T12:41:37.180362",
     "exception": false,
     "start_time": "2022-02-20T12:41:37.067833",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('train2.csv')\n",
    "test = pd.read_csv('test2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "id": "7bbacd08",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-20T12:41:37.229473Z",
     "iopub.status.busy": "2022-02-20T12:41:37.228717Z",
     "iopub.status.idle": "2022-02-20T12:41:37.241968Z",
     "shell.execute_reply": "2022-02-20T12:41:37.242495Z",
     "shell.execute_reply.started": "2022-02-20T12:38:48.390439Z"
    },
    "papermill": {
     "duration": 0.040798,
     "end_time": "2022-02-20T12:41:37.242640",
     "exception": false,
     "start_time": "2022-02-20T12:41:37.201842",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>country</th>\n",
       "      <th>store</th>\n",
       "      <th>product</th>\n",
       "      <th>num_sold</th>\n",
       "      <th>year</th>\n",
       "      <th>quarter</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>dayofweek</th>\n",
       "      <th>dayofmonth</th>\n",
       "      <th>dayofyear</th>\n",
       "      <th>weekofyear</th>\n",
       "      <th>weekday</th>\n",
       "      <th>is_weekend</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>329</td>\n",
       "      <td>2015</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>31</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>520</td>\n",
       "      <td>2015</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>31</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>146</td>\n",
       "      <td>2015</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>31</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>572</td>\n",
       "      <td>2015</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>31</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>911</td>\n",
       "      <td>2015</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>31</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   row_id  country  store  product  num_sold  year  quarter  month  day  \\\n",
       "0       0        0      0        1       329  2015        1      1    1   \n",
       "1       1        0      0        0       520  2015        1      1    1   \n",
       "2       2        0      0        2       146  2015        1      1    1   \n",
       "3       3        0      1        1       572  2015        1      1    1   \n",
       "4       4        0      1        0       911  2015        1      1    1   \n",
       "\n",
       "   dayofweek  dayofmonth  dayofyear  weekofyear  weekday  is_weekend  \n",
       "0          3          31          1           1        3           0  \n",
       "1          3          31          1           1        3           0  \n",
       "2          3          31          1           1        3           0  \n",
       "3          3          31          1           1        3           0  \n",
       "4          3          31          1           1        3           0  "
      ]
     },
     "execution_count": 462,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6052ba",
   "metadata": {
    "papermill": {
     "duration": 0.020591,
     "end_time": "2022-02-20T12:41:37.284333",
     "exception": false,
     "start_time": "2022-02-20T12:41:37.263742",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "id": "811af396",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-20T12:41:37.448718Z",
     "iopub.status.busy": "2022-02-20T12:41:37.447035Z",
     "iopub.status.idle": "2022-02-20T12:41:37.461506Z",
     "shell.execute_reply": "2022-02-20T12:41:37.461062Z",
     "shell.execute_reply.started": "2022-02-20T12:38:48.489899Z"
    },
    "papermill": {
     "duration": 0.038207,
     "end_time": "2022-02-20T12:41:37.461635",
     "exception": false,
     "start_time": "2022-02-20T12:41:37.423428",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train = train.set_index('row_id')\n",
    "test = test.set_index('row_id')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "id": "f90ba775",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train[train['num_sold']<500]\n",
    "# train = train[train['num_sold']>50]\n",
    "# train = train[['country', 'store', 'product', 'month', 'year', 'num_sold']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "id": "fe073e46",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-20T12:41:37.520646Z",
     "iopub.status.busy": "2022-02-20T12:41:37.519975Z",
     "iopub.status.idle": "2022-02-20T12:41:37.523202Z",
     "shell.execute_reply": "2022-02-20T12:41:37.523647Z",
     "shell.execute_reply.started": "2022-02-20T12:38:48.504764Z"
    },
    "papermill": {
     "duration": 0.041287,
     "end_time": "2022-02-20T12:41:37.523771",
     "exception": false,
     "start_time": "2022-02-20T12:41:37.482484",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>store</th>\n",
       "      <th>product</th>\n",
       "      <th>num_sold</th>\n",
       "      <th>year</th>\n",
       "      <th>quarter</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>dayofweek</th>\n",
       "      <th>dayofmonth</th>\n",
       "      <th>dayofyear</th>\n",
       "      <th>weekofyear</th>\n",
       "      <th>weekday</th>\n",
       "      <th>is_weekend</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>row_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>329</td>\n",
       "      <td>2015</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>31</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>520</td>\n",
       "      <td>2015</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>31</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>146</td>\n",
       "      <td>2015</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>31</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>572</td>\n",
       "      <td>2015</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>31</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>283</td>\n",
       "      <td>2015</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>31</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26288</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>351</td>\n",
       "      <td>2018</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>31</td>\n",
       "      <td>365</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26291</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>561</td>\n",
       "      <td>2018</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>31</td>\n",
       "      <td>365</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26292</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>570</td>\n",
       "      <td>2018</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>31</td>\n",
       "      <td>365</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26294</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>250</td>\n",
       "      <td>2018</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>31</td>\n",
       "      <td>365</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26297</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>388</td>\n",
       "      <td>2018</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>31</td>\n",
       "      <td>365</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21659 rows Ã— 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        country  store  product  num_sold  year  quarter  month  day  \\\n",
       "row_id                                                                 \n",
       "0             0      0        1       329  2015        1      1    1   \n",
       "1             0      0        0       520  2015        1      1    1   \n",
       "2             0      0        2       146  2015        1      1    1   \n",
       "3             0      1        1       572  2015        1      1    1   \n",
       "5             0      1        2       283  2015        1      1    1   \n",
       "...         ...    ...      ...       ...   ...      ...    ...  ...   \n",
       "26288         1      0        2       351  2018        4     12   31   \n",
       "26291         1      1        2       561  2018        4     12   31   \n",
       "26292         2      0        1       570  2018        4     12   31   \n",
       "26294         2      0        2       250  2018        4     12   31   \n",
       "26297         2      1        2       388  2018        4     12   31   \n",
       "\n",
       "        dayofweek  dayofmonth  dayofyear  weekofyear  weekday  is_weekend  \n",
       "row_id                                                                     \n",
       "0               3          31          1           1        3           0  \n",
       "1               3          31          1           1        3           0  \n",
       "2               3          31          1           1        3           0  \n",
       "3               3          31          1           1        3           0  \n",
       "5               3          31          1           1        3           0  \n",
       "...           ...         ...        ...         ...      ...         ...  \n",
       "26288           0          31        365           1        0           0  \n",
       "26291           0          31        365           1        0           0  \n",
       "26292           0          31        365           1        0           0  \n",
       "26294           0          31        365           1        0           0  \n",
       "26297           0          31        365           1        0           0  \n",
       "\n",
       "[21659 rows x 14 columns]"
      ]
     },
     "execution_count": 465,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "id": "8ad262c7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-20T12:41:37.699495Z",
     "iopub.status.busy": "2022-02-20T12:41:37.698651Z",
     "iopub.status.idle": "2022-02-20T12:41:37.705486Z",
     "shell.execute_reply": "2022-02-20T12:41:37.705034Z",
     "shell.execute_reply.started": "2022-02-20T12:38:48.586945Z"
    },
    "papermill": {
     "duration": 0.032043,
     "end_time": "2022-02-20T12:41:37.705600",
     "exception": false,
     "start_time": "2022-02-20T12:41:37.673557",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "x_data = train.drop('num_sold', axis=1)\n",
    "# y_data = train.num_sold\n",
    "# std = StandardScaler()\n",
    "# x_data = std.fit_transform(x_data)\n",
    "# test = std.transform(test)\n",
    "\n",
    "test = torch.tensor(test.values)\n",
    "y_data = train[['num_sold']]\n",
    "x_data = torch.tensor(x_data.values)\n",
    "y_data = torch.tensor(y_data.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "id": "8430728e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 1,  ..., 1, 3, 0],\n",
       "        [0, 0, 0,  ..., 1, 3, 0],\n",
       "        [0, 0, 2,  ..., 1, 3, 0],\n",
       "        ...,\n",
       "        [2, 0, 1,  ..., 1, 0, 0],\n",
       "        [2, 0, 2,  ..., 1, 0, 0],\n",
       "        [2, 1, 2,  ..., 1, 0, 0]])"
      ]
     },
     "execution_count": 467,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "id": "bcc55061",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-20T12:41:37.754216Z",
     "iopub.status.busy": "2022-02-20T12:41:37.753640Z",
     "iopub.status.idle": "2022-02-20T12:41:37.769177Z",
     "shell.execute_reply": "2022-02-20T12:41:37.768753Z",
     "shell.execute_reply.started": "2022-02-20T12:38:48.600314Z"
    },
    "papermill": {
     "duration": 0.0422,
     "end_time": "2022-02-20T12:41:37.769310",
     "exception": false,
     "start_time": "2022-02-20T12:41:37.727110",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\willi\\AppData\\Local\\Temp\\ipykernel_24376\\1023721312.py:1: UserWarning:\n",
      "\n",
      "To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "\n",
      "C:\\Users\\willi\\AppData\\Local\\Temp\\ipykernel_24376\\1023721312.py:2: UserWarning:\n",
      "\n",
      "To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "\n",
      "C:\\Users\\willi\\AppData\\Local\\Temp\\ipykernel_24376\\1023721312.py:4: UserWarning:\n",
      "\n",
      "To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x_data = torch.tensor(x_data, dtype=torch.float32)\n",
    "test = torch.tensor(test, dtype=torch.float32)\n",
    "\n",
    "y_data = torch.tensor(y_data, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "id": "47324a6d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-20T12:41:37.819741Z",
     "iopub.status.busy": "2022-02-20T12:41:37.819194Z",
     "iopub.status.idle": "2022-02-20T12:41:37.865113Z",
     "shell.execute_reply": "2022-02-20T12:41:37.865628Z",
     "shell.execute_reply.started": "2022-02-20T12:38:48.611550Z"
    },
    "papermill": {
     "duration": 0.074923,
     "end_time": "2022-02-20T12:41:37.865759",
     "exception": false,
     "start_time": "2022-02-20T12:41:37.790836",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0000e+00, 0.0000e+00, 1.0000e+00, 2.0150e+03, 1.0000e+00, 1.0000e+00,\n",
      "         1.0000e+00, 3.0000e+00, 3.1000e+01, 1.0000e+00, 1.0000e+00, 3.0000e+00,\n",
      "         0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 2.0150e+03, 1.0000e+00, 1.0000e+00,\n",
      "         1.0000e+00, 3.0000e+00, 3.1000e+01, 1.0000e+00, 1.0000e+00, 3.0000e+00,\n",
      "         0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 2.0000e+00, 2.0150e+03, 1.0000e+00, 1.0000e+00,\n",
      "         1.0000e+00, 3.0000e+00, 3.1000e+01, 1.0000e+00, 1.0000e+00, 3.0000e+00,\n",
      "         0.0000e+00],\n",
      "        [0.0000e+00, 1.0000e+00, 1.0000e+00, 2.0150e+03, 1.0000e+00, 1.0000e+00,\n",
      "         1.0000e+00, 3.0000e+00, 3.1000e+01, 1.0000e+00, 1.0000e+00, 3.0000e+00,\n",
      "         0.0000e+00],\n",
      "        [0.0000e+00, 1.0000e+00, 2.0000e+00, 2.0150e+03, 1.0000e+00, 1.0000e+00,\n",
      "         1.0000e+00, 3.0000e+00, 3.1000e+01, 1.0000e+00, 1.0000e+00, 3.0000e+00,\n",
      "         0.0000e+00],\n",
      "        [1.0000e+00, 0.0000e+00, 1.0000e+00, 2.0150e+03, 1.0000e+00, 1.0000e+00,\n",
      "         1.0000e+00, 3.0000e+00, 3.1000e+01, 1.0000e+00, 1.0000e+00, 3.0000e+00,\n",
      "         0.0000e+00],\n",
      "        [1.0000e+00, 0.0000e+00, 2.0000e+00, 2.0150e+03, 1.0000e+00, 1.0000e+00,\n",
      "         1.0000e+00, 3.0000e+00, 3.1000e+01, 1.0000e+00, 1.0000e+00, 3.0000e+00,\n",
      "         0.0000e+00],\n",
      "        [1.0000e+00, 1.0000e+00, 2.0000e+00, 2.0150e+03, 1.0000e+00, 1.0000e+00,\n",
      "         1.0000e+00, 3.0000e+00, 3.1000e+01, 1.0000e+00, 1.0000e+00, 3.0000e+00,\n",
      "         0.0000e+00],\n",
      "        [2.0000e+00, 0.0000e+00, 1.0000e+00, 2.0150e+03, 1.0000e+00, 1.0000e+00,\n",
      "         1.0000e+00, 3.0000e+00, 3.1000e+01, 1.0000e+00, 1.0000e+00, 3.0000e+00,\n",
      "         0.0000e+00],\n",
      "        [2.0000e+00, 0.0000e+00, 2.0000e+00, 2.0150e+03, 1.0000e+00, 1.0000e+00,\n",
      "         1.0000e+00, 3.0000e+00, 3.1000e+01, 1.0000e+00, 1.0000e+00, 3.0000e+00,\n",
      "         0.0000e+00]])\n",
      "torch.Size([21659, 13])\n"
     ]
    }
   ],
   "source": [
    "print(x_data[:10])\n",
    "\n",
    "print(x_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd91dfc",
   "metadata": {
    "papermill": {
     "duration": 0.02154,
     "end_time": "2022-02-20T12:41:37.910220",
     "exception": false,
     "start_time": "2022-02-20T12:41:37.888680",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### model train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "id": "4e74e27d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-20T12:41:37.959536Z",
     "iopub.status.busy": "2022-02-20T12:41:37.957296Z",
     "iopub.status.idle": "2022-02-20T12:41:37.965051Z",
     "shell.execute_reply": "2022-02-20T12:41:37.965494Z",
     "shell.execute_reply.started": "2022-02-20T12:38:48.625064Z"
    },
    "papermill": {
     "duration": 0.033781,
     "end_time": "2022-02-20T12:41:37.965638",
     "exception": false,
     "start_time": "2022-02-20T12:41:37.931857",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x_data, y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "id": "95b00af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.unique(y_train.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "id": "97dc5502",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-20T12:41:38.013609Z",
     "iopub.status.busy": "2022-02-20T12:41:38.013053Z",
     "iopub.status.idle": "2022-02-20T12:41:38.016774Z",
     "shell.execute_reply": "2022-02-20T12:41:38.016362Z",
     "shell.execute_reply.started": "2022-02-20T12:38:48.637980Z"
    },
    "papermill": {
     "duration": 0.029146,
     "end_time": "2022-02-20T12:41:38.016883",
     "exception": false,
     "start_time": "2022-02-20T12:41:37.987737",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "id": "aecd7244",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-20T12:41:38.066581Z",
     "iopub.status.busy": "2022-02-20T12:41:38.064668Z",
     "iopub.status.idle": "2022-02-20T12:41:38.067258Z",
     "shell.execute_reply": "2022-02-20T12:41:38.067749Z",
     "shell.execute_reply.started": "2022-02-20T12:38:48.646390Z"
    },
    "papermill": {
     "duration": 0.029335,
     "end_time": "2022-02-20T12:41:38.067920",
     "exception": false,
     "start_time": "2022-02-20T12:41:38.038585",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "id": "1cd6b6f4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-20T12:41:38.117101Z",
     "iopub.status.busy": "2022-02-20T12:41:38.116615Z",
     "iopub.status.idle": "2022-02-20T12:41:38.120221Z",
     "shell.execute_reply": "2022-02-20T12:41:38.119727Z",
     "shell.execute_reply.started": "2022-02-20T12:38:48.655705Z"
    },
    "papermill": {
     "duration": 0.030349,
     "end_time": "2022-02-20T12:41:38.120339",
     "exception": false,
     "start_time": "2022-02-20T12:41:38.089990",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_train = TensorDataset(x_train, y_train)\n",
    "loader_train = DataLoader(dataset_train, batch_size=256, shuffle=True)\n",
    "\n",
    "dataset_test = TensorDataset(x_test, y_test)\n",
    "loader_test = DataLoader(dataset_test, batch_size=256, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "id": "4a1683b1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-20T12:41:38.207955Z",
     "iopub.status.busy": "2022-02-20T12:41:38.166378Z",
     "iopub.status.idle": "2022-02-20T12:41:38.211552Z",
     "shell.execute_reply": "2022-02-20T12:41:38.211079Z",
     "shell.execute_reply.started": "2022-02-20T12:38:48.666773Z"
    },
    "papermill": {
     "duration": 0.069728,
     "end_time": "2022-02-20T12:41:38.211681",
     "exception": false,
     "start_time": "2022-02-20T12:41:38.141953",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Using {device} device')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "id": "e5595e90",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-20T12:41:38.264868Z",
     "iopub.status.busy": "2022-02-20T12:41:38.263296Z",
     "iopub.status.idle": "2022-02-20T12:41:38.265498Z",
     "shell.execute_reply": "2022-02-20T12:41:38.265911Z",
     "shell.execute_reply.started": "2022-02-20T12:38:48.680752Z"
    },
    "papermill": {
     "duration": 0.032021,
     "end_time": "2022-02-20T12:41:38.266029",
     "exception": false,
     "start_time": "2022-02-20T12:41:38.234008",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "n_feature = 13\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(n_feature, n_feature*2)\n",
    "        self.fc2 = nn.Linear(n_feature*2, n_feature*4)\n",
    "        self.bn1 = nn.BatchNorm1d(n_feature*4)\n",
    "        self.fc3 = nn.Linear(n_feature*4, n_feature*8)\n",
    "        self.fc4 = nn.Linear(n_feature*8, n_feature*16)\n",
    "        # self.bn2 = nn.BatchNorm1d(n_feature*16)\n",
    "        # self.fc5 = nn.Linear(n_feature*16, n_feature*32)\n",
    "        # self.fc6 = nn.Linear(n_feature*32, n_feature*64)\n",
    "        # self.fc7 = nn.Linear(n_feature*64, n_feature*128)\n",
    "        # self.bn3 = nn.BatchNorm1d(n_feature*128)\n",
    "        # self.fc8 = nn.Linear(n_feature*128, n_feature*64)\n",
    "        # self.fc9 = nn.Linear(n_feature*64, n_feature*32)\n",
    "        # self.fc10 = nn.Linear(n_feature*32, n_feature*16)\n",
    "        # self.bn4 = nn.BatchNorm1d(n_feature*16)\n",
    "        # self.fc11 = nn.Linear(n_feature*16, n_feature*8)\n",
    "        # self.fc12 = nn.Linear(n_feature*8, n_feature*4)\n",
    "        # self.bn5 = nn.BatchNorm1d(n_feature*4)\n",
    "        # self.fc13 = nn.Linear(n_feature*4, n_feature*2)\n",
    "        # self.fc14 = nn.Linear(n_feature*2, n_feature)\n",
    "        self.fc15 = nn.Linear(n_feature*16, 1)\n",
    "\n",
    "        # self.fc1 = nn.Linear(n_feature, n_feature*2)\n",
    "        # self.bn1 = nn.BatchNorm1d(n_feature*2)\n",
    "        # self.fc2 = nn.Linear(n_feature*2, n_feature*4)\n",
    "        # self.bn2 = nn.BatchNorm1d(n_feature*4)\n",
    "        # self.fc3 = nn.Linear(n_feature*4, n_feature*8)\n",
    "        # self.bn3 = nn.BatchNorm1d(n_feature*8)\n",
    "        # self.fc4 = nn.Linear(n_feature*8, n_feature*4)\n",
    "        # self.fc5 = nn.Linear(n_feature*4, n_feature*2)\n",
    "        # self.bn5 = nn.BatchNorm1d(n_feature*2)\n",
    "        # self.fc6 = nn.Linear(n_feature*2, n_feature)\n",
    "        # self.fc7 = nn.Linear(n_feature,  1)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.bn1(x)\n",
    "\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        # x = self.bn2(x)\n",
    "        # x = F.relu(self.fc5(x)) \n",
    "        # x = F.relu(self.fc6(x) )\n",
    "        # x = F.relu(self.fc7(x) )\n",
    "        # x = self.bn3(x) \n",
    "        # x = F.relu(self.fc8(x) )\n",
    "        # x = F.relu(self.fc9(x) )\n",
    "        # x = F.relu(self.fc10(x) )\n",
    "        # x = self.bn4(x)\n",
    "        # x = F.relu(self.fc11(x) )\n",
    "        # x = F.relu(self.fc12(x) )\n",
    "        # x = self.bn5(x) \n",
    "        # x = F.relu(self.fc13(x) )\n",
    "        # x = F.relu(self.fc14(x))\n",
    "        x = self.fc15(x) \n",
    "        \n",
    "        # x = self.fc1(x)\n",
    "        # x = F.relu(self.bn1(x))\n",
    "        # x = F.relu(self.fc2(x))\n",
    "        # x= F.relu(self.bn2(x) )\n",
    "        # x= F.relu(self.fc3(x) )\n",
    "        # x= F.relu(self.bn3(x) )\n",
    "        # x= F.relu(self.fc4(x) )\n",
    "        # x = F.relu(self.fc5(x))\n",
    "        # x = F.relu(self.bn5(x))\n",
    "        # x = F.relu(self.fc6(x))\n",
    "        # x = self.fc7(x)\n",
    "        # x = self.fc8(x)\n",
    "\n",
    "\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "id": "016bc555",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-20T12:41:38.315229Z",
     "iopub.status.busy": "2022-02-20T12:41:38.314648Z",
     "iopub.status.idle": "2022-02-20T12:41:41.077687Z",
     "shell.execute_reply": "2022-02-20T12:41:41.078302Z",
     "shell.execute_reply.started": "2022-02-20T12:38:48.693264Z"
    },
    "papermill": {
     "duration": 2.790367,
     "end_time": "2022-02-20T12:41:41.078501",
     "exception": false,
     "start_time": "2022-02-20T12:41:38.288134",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (fc1): Linear(in_features=13, out_features=26, bias=True)\n",
      "  (fc2): Linear(in_features=26, out_features=52, bias=True)\n",
      "  (bn1): BatchNorm1d(52, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc3): Linear(in_features=52, out_features=104, bias=True)\n",
      "  (fc4): Linear(in_features=104, out_features=208, bias=True)\n",
      "  (fc15): Linear(in_features=208, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = NeuralNetwork().to(device)\n",
    "# model.apply(init_weights)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "id": "b786044f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-20T12:41:41.129741Z",
     "iopub.status.busy": "2022-02-20T12:41:41.129008Z",
     "iopub.status.idle": "2022-02-20T12:41:41.130935Z",
     "shell.execute_reply": "2022-02-20T12:41:41.131337Z",
     "shell.execute_reply.started": "2022-02-20T12:38:48.713444Z"
    },
    "papermill": {
     "duration": 0.029138,
     "end_time": "2022-02-20T12:41:41.131471",
     "exception": false,
     "start_time": "2022-02-20T12:41:41.102333",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "batch_size = 2\n",
    "epochs = 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "id": "bae0821d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-20T12:41:41.184743Z",
     "iopub.status.busy": "2022-02-20T12:41:41.183994Z",
     "iopub.status.idle": "2022-02-20T12:41:41.186437Z",
     "shell.execute_reply": "2022-02-20T12:41:41.186010Z",
     "shell.execute_reply.started": "2022-02-20T12:38:48.724277Z"
    },
    "papermill": {
     "duration": 0.032904,
     "end_time": "2022-02-20T12:41:41.186548",
     "exception": false,
     "start_time": "2022-02-20T12:41:41.153644",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        \n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # scheduler.step()\n",
    "        optimizer.step()\n",
    "\n",
    "    loss = loss.item()\n",
    "    loss = loss ** 0.5\n",
    "    print(f\"loss: {loss:>7f}\")\n",
    "\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            \n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    test_loss = test_loss ** 0.5\n",
    "    print(f\"Test Error: \\n Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "id": "7bfe4a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import exp\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "id": "bd144b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cyclical_lr(stepsize, min_lr=3e-4, max_lr=3e-3):\n",
    "    \n",
    "    # Scaler: we can adapt this if we do not want the triangular CLR\n",
    "    scaler = lambda x: 1.\n",
    "\n",
    "    # Lambda function to calculate the LR\n",
    "    lr_lambda = lambda it: min_lr + (max_lr - min_lr) * relative(it, stepsize)\n",
    "\n",
    "    # Additional function to see where on the cycle we are\n",
    "    def relative(it, stepsize):\n",
    "        cycle = math.floor(1 + it / (2 * stepsize))\n",
    "        x = abs(it / stepsize - 2 * cycle + 1)\n",
    "        return max(0, (1 - x)) * scaler(cycle)\n",
    "\n",
    "    return lr_lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "id": "d3d58a0f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-20T12:41:41.236469Z",
     "iopub.status.busy": "2022-02-20T12:41:41.235787Z",
     "iopub.status.idle": "2022-02-20T12:42:09.077192Z",
     "shell.execute_reply": "2022-02-20T12:42:09.077754Z",
     "shell.execute_reply.started": "2022-02-20T12:39:38.853750Z"
    },
    "papermill": {
     "duration": 27.869117,
     "end_time": "2022-02-20T12:42:09.077955",
     "exception": false,
     "start_time": "2022-02-20T12:41:41.208838",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 11.469870\n",
      "Test Error: \n",
      " Avg loss: 11.490527 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 10.942806\n",
      "Test Error: \n",
      " Avg loss: 10.745648 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 11.198273\n",
      "Test Error: \n",
      " Avg loss: 10.642089 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 10.423350\n",
      "Test Error: \n",
      " Avg loss: 10.715424 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 9.983295\n",
      "Test Error: \n",
      " Avg loss: 10.710802 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 10.466375\n",
      "Test Error: \n",
      " Avg loss: 10.686974 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 10.847547\n",
      "Test Error: \n",
      " Avg loss: 10.668442 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 11.585630\n",
      "Test Error: \n",
      " Avg loss: 10.611988 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 10.770931\n",
      "Test Error: \n",
      " Avg loss: 10.683534 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 10.871387\n",
      "Test Error: \n",
      " Avg loss: 10.662604 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 11.259206\n",
      "Test Error: \n",
      " Avg loss: 10.701781 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 10.807832\n",
      "Test Error: \n",
      " Avg loss: 10.553075 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 10.784195\n",
      "Test Error: \n",
      " Avg loss: 11.021391 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 11.026368\n",
      "Test Error: \n",
      " Avg loss: 10.747609 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 10.517266\n",
      "Test Error: \n",
      " Avg loss: 10.772519 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 10.605208\n",
      "Test Error: \n",
      " Avg loss: 10.687945 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 10.750118\n",
      "Test Error: \n",
      " Avg loss: 10.718334 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 11.317173\n",
      "Test Error: \n",
      " Avg loss: 10.738271 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 10.651251\n",
      "Test Error: \n",
      " Avg loss: 10.712766 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 10.697368\n",
      "Test Error: \n",
      " Avg loss: 10.745784 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 10.687606\n",
      "Test Error: \n",
      " Avg loss: 10.681511 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 10.030434\n",
      "Test Error: \n",
      " Avg loss: 10.681356 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss: 10.860219\n",
      "Test Error: \n",
      " Avg loss: 10.908050 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "loss: 10.783380\n",
      "Test Error: \n",
      " Avg loss: 10.986656 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "loss: 10.912677\n",
      "Test Error: \n",
      " Avg loss: 11.637939 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "loss: 10.979867\n",
      "Test Error: \n",
      " Avg loss: 10.698499 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "loss: 10.981117\n",
      "Test Error: \n",
      " Avg loss: 10.685635 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "loss: 10.605451\n",
      "Test Error: \n",
      " Avg loss: 10.891328 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "loss: 10.371314\n",
      "Test Error: \n",
      " Avg loss: 10.731027 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "loss: 10.689085\n",
      "Test Error: \n",
      " Avg loss: 10.714459 \n",
      "\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "loss: 10.656156\n",
      "Test Error: \n",
      " Avg loss: 11.063773 \n",
      "\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "loss: 11.050096\n",
      "Test Error: \n",
      " Avg loss: 10.729969 \n",
      "\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "loss: 10.469538\n",
      "Test Error: \n",
      " Avg loss: 10.676926 \n",
      "\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "loss: 11.176241\n",
      "Test Error: \n",
      " Avg loss: 10.747421 \n",
      "\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "loss: 10.354665\n",
      "Test Error: \n",
      " Avg loss: 10.722899 \n",
      "\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "loss: 10.939504\n",
      "Test Error: \n",
      " Avg loss: 10.792669 \n",
      "\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "loss: 10.265671\n",
      "Test Error: \n",
      " Avg loss: 10.747811 \n",
      "\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "loss: 10.589347\n",
      "Test Error: \n",
      " Avg loss: 10.714766 \n",
      "\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "loss: 10.693810\n",
      "Test Error: \n",
      " Avg loss: 10.714343 \n",
      "\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "loss: 10.202071\n",
      "Test Error: \n",
      " Avg loss: 10.692736 \n",
      "\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "loss: 10.373911\n",
      "Test Error: \n",
      " Avg loss: 10.716430 \n",
      "\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "loss: 10.625625\n",
      "Test Error: \n",
      " Avg loss: 10.694700 \n",
      "\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "loss: 11.262400\n",
      "Test Error: \n",
      " Avg loss: 10.710611 \n",
      "\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "loss: 10.861527\n",
      "Test Error: \n",
      " Avg loss: 10.786565 \n",
      "\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "loss: 11.056057\n",
      "Test Error: \n",
      " Avg loss: 10.770175 \n",
      "\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "loss: 10.665689\n",
      "Test Error: \n",
      " Avg loss: 10.680774 \n",
      "\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "loss: 10.707643\n",
      "Test Error: \n",
      " Avg loss: 10.720689 \n",
      "\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "loss: 10.437714\n",
      "Test Error: \n",
      " Avg loss: 10.736758 \n",
      "\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "loss: 9.921437\n",
      "Test Error: \n",
      " Avg loss: 10.866780 \n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "loss: 10.237000\n",
      "Test Error: \n",
      " Avg loss: 10.723770 \n",
      "\n",
      "Epoch 51\n",
      "-------------------------------\n",
      "loss: 10.313368\n",
      "Test Error: \n",
      " Avg loss: 10.698359 \n",
      "\n",
      "Epoch 52\n",
      "-------------------------------\n",
      "loss: 10.994312\n",
      "Test Error: \n",
      " Avg loss: 10.729078 \n",
      "\n",
      "Epoch 53\n",
      "-------------------------------\n",
      "loss: 10.646950\n",
      "Test Error: \n",
      " Avg loss: 10.730282 \n",
      "\n",
      "Epoch 54\n",
      "-------------------------------\n",
      "loss: 10.755370\n",
      "Test Error: \n",
      " Avg loss: 10.687566 \n",
      "\n",
      "Epoch 55\n",
      "-------------------------------\n",
      "loss: 10.833973\n",
      "Test Error: \n",
      " Avg loss: 10.752944 \n",
      "\n",
      "Epoch 56\n",
      "-------------------------------\n",
      "loss: 10.952653\n",
      "Test Error: \n",
      " Avg loss: 10.686264 \n",
      "\n",
      "Epoch 57\n",
      "-------------------------------\n",
      "loss: 11.134956\n",
      "Test Error: \n",
      " Avg loss: 10.744646 \n",
      "\n",
      "Epoch 58\n",
      "-------------------------------\n",
      "loss: 10.350336\n",
      "Test Error: \n",
      " Avg loss: 10.746066 \n",
      "\n",
      "Epoch 59\n",
      "-------------------------------\n",
      "loss: 10.623261\n",
      "Test Error: \n",
      " Avg loss: 10.704831 \n",
      "\n",
      "Epoch 60\n",
      "-------------------------------\n",
      "loss: 10.613503\n",
      "Test Error: \n",
      " Avg loss: 10.772373 \n",
      "\n",
      "Epoch 61\n",
      "-------------------------------\n",
      "loss: 10.422375\n",
      "Test Error: \n",
      " Avg loss: 10.724997 \n",
      "\n",
      "Epoch 62\n",
      "-------------------------------\n",
      "loss: 10.660221\n",
      "Test Error: \n",
      " Avg loss: 10.974833 \n",
      "\n",
      "Epoch 63\n",
      "-------------------------------\n",
      "loss: 10.527641\n",
      "Test Error: \n",
      " Avg loss: 10.772344 \n",
      "\n",
      "Epoch 64\n",
      "-------------------------------\n",
      "loss: 11.390047\n",
      "Test Error: \n",
      " Avg loss: 10.968738 \n",
      "\n",
      "Epoch 65\n",
      "-------------------------------\n",
      "loss: 10.959580\n",
      "Test Error: \n",
      " Avg loss: 10.652632 \n",
      "\n",
      "Epoch 66\n",
      "-------------------------------\n",
      "loss: 10.662275\n",
      "Test Error: \n",
      " Avg loss: 10.664972 \n",
      "\n",
      "Epoch 67\n",
      "-------------------------------\n",
      "loss: 10.082365\n",
      "Test Error: \n",
      " Avg loss: 10.720947 \n",
      "\n",
      "Epoch 68\n",
      "-------------------------------\n",
      "loss: 10.648686\n",
      "Test Error: \n",
      " Avg loss: 10.733643 \n",
      "\n",
      "Epoch 69\n",
      "-------------------------------\n",
      "loss: 10.706484\n",
      "Test Error: \n",
      " Avg loss: 10.698051 \n",
      "\n",
      "Epoch 70\n",
      "-------------------------------\n",
      "loss: 10.802390\n",
      "Test Error: \n",
      " Avg loss: 10.845690 \n",
      "\n",
      "Epoch 71\n",
      "-------------------------------\n",
      "loss: 10.726689\n",
      "Test Error: \n",
      " Avg loss: 10.864135 \n",
      "\n",
      "Epoch 72\n",
      "-------------------------------\n",
      "loss: 10.268994\n",
      "Test Error: \n",
      " Avg loss: 10.716968 \n",
      "\n",
      "Epoch 73\n",
      "-------------------------------\n",
      "loss: 10.507133\n",
      "Test Error: \n",
      " Avg loss: 10.697961 \n",
      "\n",
      "Epoch 74\n",
      "-------------------------------\n",
      "loss: 10.960445\n",
      "Test Error: \n",
      " Avg loss: 10.745377 \n",
      "\n",
      "Epoch 75\n",
      "-------------------------------\n",
      "loss: 10.513167\n",
      "Test Error: \n",
      " Avg loss: 10.742497 \n",
      "\n",
      "Epoch 76\n",
      "-------------------------------\n",
      "loss: 11.126133\n",
      "Test Error: \n",
      " Avg loss: 11.032167 \n",
      "\n",
      "Epoch 77\n",
      "-------------------------------\n",
      "loss: 10.227663\n",
      "Test Error: \n",
      " Avg loss: 10.764139 \n",
      "\n",
      "Epoch 78\n",
      "-------------------------------\n",
      "loss: 11.275198\n",
      "Test Error: \n",
      " Avg loss: 10.751330 \n",
      "\n",
      "Epoch 79\n",
      "-------------------------------\n",
      "loss: 11.014680\n",
      "Test Error: \n",
      " Avg loss: 10.697391 \n",
      "\n",
      "Epoch 80\n",
      "-------------------------------\n",
      "loss: 10.387516\n",
      "Test Error: \n",
      " Avg loss: 10.725760 \n",
      "\n",
      "Epoch 81\n",
      "-------------------------------\n",
      "loss: 10.975728\n",
      "Test Error: \n",
      " Avg loss: 10.715219 \n",
      "\n",
      "Epoch 82\n",
      "-------------------------------\n",
      "loss: 10.801399\n",
      "Test Error: \n",
      " Avg loss: 10.849426 \n",
      "\n",
      "Epoch 83\n",
      "-------------------------------\n",
      "loss: 11.012658\n",
      "Test Error: \n",
      " Avg loss: 10.851219 \n",
      "\n",
      "Epoch 84\n",
      "-------------------------------\n",
      "loss: 10.786501\n",
      "Test Error: \n",
      " Avg loss: 10.662670 \n",
      "\n",
      "Epoch 85\n",
      "-------------------------------\n",
      "loss: 10.832199\n",
      "Test Error: \n",
      " Avg loss: 10.840976 \n",
      "\n",
      "Epoch 86\n",
      "-------------------------------\n",
      "loss: 10.719686\n",
      "Test Error: \n",
      " Avg loss: 10.712649 \n",
      "\n",
      "Epoch 87\n",
      "-------------------------------\n",
      "loss: 10.456855\n",
      "Test Error: \n",
      " Avg loss: 10.730480 \n",
      "\n",
      "Epoch 88\n",
      "-------------------------------\n",
      "loss: 10.800143\n",
      "Test Error: \n",
      " Avg loss: 10.733973 \n",
      "\n",
      "Epoch 89\n",
      "-------------------------------\n",
      "loss: 10.737294\n",
      "Test Error: \n",
      " Avg loss: 10.765441 \n",
      "\n",
      "Epoch 90\n",
      "-------------------------------\n",
      "loss: 10.701762\n",
      "Test Error: \n",
      " Avg loss: 10.852855 \n",
      "\n",
      "Epoch 91\n",
      "-------------------------------\n",
      "loss: 10.461502\n",
      "Test Error: \n",
      " Avg loss: 10.676211 \n",
      "\n",
      "Epoch 92\n",
      "-------------------------------\n",
      "loss: 10.679723\n",
      "Test Error: \n",
      " Avg loss: 10.753939 \n",
      "\n",
      "Epoch 93\n",
      "-------------------------------\n",
      "loss: 10.762250\n",
      "Test Error: \n",
      " Avg loss: 10.802425 \n",
      "\n",
      "Epoch 94\n",
      "-------------------------------\n",
      "loss: 10.551620\n",
      "Test Error: \n",
      " Avg loss: 10.718190 \n",
      "\n",
      "Epoch 95\n",
      "-------------------------------\n",
      "loss: 11.022511\n",
      "Test Error: \n",
      " Avg loss: 10.686898 \n",
      "\n",
      "Epoch 96\n",
      "-------------------------------\n",
      "loss: 10.379156\n",
      "Test Error: \n",
      " Avg loss: 10.795601 \n",
      "\n",
      "Epoch 97\n",
      "-------------------------------\n",
      "loss: 10.195141\n",
      "Test Error: \n",
      " Avg loss: 10.719265 \n",
      "\n",
      "Epoch 98\n",
      "-------------------------------\n",
      "loss: 10.529285\n",
      "Test Error: \n",
      " Avg loss: 10.727924 \n",
      "\n",
      "Epoch 99\n",
      "-------------------------------\n",
      "loss: 10.737168\n",
      "Test Error: \n",
      " Avg loss: 10.967549 \n",
      "\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "loss: 10.915319\n",
      "Test Error: \n",
      " Avg loss: 10.698817 \n",
      "\n",
      "Epoch 101\n",
      "-------------------------------\n",
      "loss: 10.235294\n",
      "Test Error: \n",
      " Avg loss: 11.157066 \n",
      "\n",
      "Epoch 102\n",
      "-------------------------------\n",
      "loss: 11.060548\n",
      "Test Error: \n",
      " Avg loss: 10.712361 \n",
      "\n",
      "Epoch 103\n",
      "-------------------------------\n",
      "loss: 10.447445\n",
      "Test Error: \n",
      " Avg loss: 10.753706 \n",
      "\n",
      "Epoch 104\n",
      "-------------------------------\n",
      "loss: 11.245876\n",
      "Test Error: \n",
      " Avg loss: 10.701987 \n",
      "\n",
      "Epoch 105\n",
      "-------------------------------\n",
      "loss: 10.330047\n",
      "Test Error: \n",
      " Avg loss: 10.804587 \n",
      "\n",
      "Epoch 106\n",
      "-------------------------------\n",
      "loss: 10.688232\n",
      "Test Error: \n",
      " Avg loss: 10.684215 \n",
      "\n",
      "Epoch 107\n",
      "-------------------------------\n",
      "loss: 10.555192\n",
      "Test Error: \n",
      " Avg loss: 10.758528 \n",
      "\n",
      "Epoch 108\n",
      "-------------------------------\n",
      "loss: 10.925446\n",
      "Test Error: \n",
      " Avg loss: 10.693585 \n",
      "\n",
      "Epoch 109\n",
      "-------------------------------\n",
      "loss: 10.767503\n",
      "Test Error: \n",
      " Avg loss: 10.699040 \n",
      "\n",
      "Epoch 110\n",
      "-------------------------------\n",
      "loss: 10.912884\n",
      "Test Error: \n",
      " Avg loss: 10.723076 \n",
      "\n",
      "Epoch 111\n",
      "-------------------------------\n",
      "loss: 10.916370\n",
      "Test Error: \n",
      " Avg loss: 10.685872 \n",
      "\n",
      "Epoch 112\n",
      "-------------------------------\n",
      "loss: 10.814663\n",
      "Test Error: \n",
      " Avg loss: 10.732078 \n",
      "\n",
      "Epoch 113\n",
      "-------------------------------\n",
      "loss: 10.747221\n",
      "Test Error: \n",
      " Avg loss: 10.676339 \n",
      "\n",
      "Epoch 114\n",
      "-------------------------------\n",
      "loss: 10.607131\n",
      "Test Error: \n",
      " Avg loss: 10.743996 \n",
      "\n",
      "Epoch 115\n",
      "-------------------------------\n",
      "loss: 10.284913\n",
      "Test Error: \n",
      " Avg loss: 10.733420 \n",
      "\n",
      "Epoch 116\n",
      "-------------------------------\n",
      "loss: 10.123649\n",
      "Test Error: \n",
      " Avg loss: 10.822988 \n",
      "\n",
      "Epoch 117\n",
      "-------------------------------\n",
      "loss: 10.330375\n",
      "Test Error: \n",
      " Avg loss: 10.752231 \n",
      "\n",
      "Epoch 118\n",
      "-------------------------------\n",
      "loss: 10.734423\n",
      "Test Error: \n",
      " Avg loss: 10.745336 \n",
      "\n",
      "Epoch 119\n",
      "-------------------------------\n",
      "loss: 10.450822\n",
      "Test Error: \n",
      " Avg loss: 10.729204 \n",
      "\n",
      "Epoch 120\n",
      "-------------------------------\n",
      "loss: 10.440108\n",
      "Test Error: \n",
      " Avg loss: 10.732494 \n",
      "\n",
      "Epoch 121\n",
      "-------------------------------\n",
      "loss: 11.217930\n",
      "Test Error: \n",
      " Avg loss: 10.694364 \n",
      "\n",
      "Epoch 122\n",
      "-------------------------------\n",
      "loss: 10.358113\n",
      "Test Error: \n",
      " Avg loss: 10.751454 \n",
      "\n",
      "Epoch 123\n",
      "-------------------------------\n",
      "loss: 10.575415\n",
      "Test Error: \n",
      " Avg loss: 10.727461 \n",
      "\n",
      "Epoch 124\n",
      "-------------------------------\n",
      "loss: 11.089396\n",
      "Test Error: \n",
      " Avg loss: 10.724641 \n",
      "\n",
      "Epoch 125\n",
      "-------------------------------\n",
      "loss: 10.563707\n",
      "Test Error: \n",
      " Avg loss: 10.707620 \n",
      "\n",
      "Epoch 126\n",
      "-------------------------------\n",
      "loss: 10.679395\n",
      "Test Error: \n",
      " Avg loss: 10.750084 \n",
      "\n",
      "Epoch 127\n",
      "-------------------------------\n",
      "loss: 10.300384\n",
      "Test Error: \n",
      " Avg loss: 10.756721 \n",
      "\n",
      "Epoch 128\n",
      "-------------------------------\n",
      "loss: 10.788129\n",
      "Test Error: \n",
      " Avg loss: 10.764942 \n",
      "\n",
      "Epoch 129\n",
      "-------------------------------\n",
      "loss: 10.555435\n",
      "Test Error: \n",
      " Avg loss: 10.798377 \n",
      "\n",
      "Epoch 130\n",
      "-------------------------------\n",
      "loss: 10.916291\n",
      "Test Error: \n",
      " Avg loss: 10.703673 \n",
      "\n",
      "Epoch 131\n",
      "-------------------------------\n",
      "loss: 10.426497\n",
      "Test Error: \n",
      " Avg loss: 10.745338 \n",
      "\n",
      "Epoch 132\n",
      "-------------------------------\n",
      "loss: 10.672034\n",
      "Test Error: \n",
      " Avg loss: 10.677442 \n",
      "\n",
      "Epoch 133\n",
      "-------------------------------\n",
      "loss: 10.124999\n",
      "Test Error: \n",
      " Avg loss: 10.730627 \n",
      "\n",
      "Epoch 134\n",
      "-------------------------------\n",
      "loss: 10.224542\n",
      "Test Error: \n",
      " Avg loss: 10.705404 \n",
      "\n",
      "Epoch 135\n",
      "-------------------------------\n",
      "loss: 10.939705\n",
      "Test Error: \n",
      " Avg loss: 10.690082 \n",
      "\n",
      "Epoch 136\n",
      "-------------------------------\n",
      "loss: 10.711182\n",
      "Test Error: \n",
      " Avg loss: 10.849088 \n",
      "\n",
      "Epoch 137\n",
      "-------------------------------\n",
      "loss: 10.818407\n",
      "Test Error: \n",
      " Avg loss: 10.804319 \n",
      "\n",
      "Epoch 138\n",
      "-------------------------------\n",
      "loss: 11.072065\n",
      "Test Error: \n",
      " Avg loss: 10.666476 \n",
      "\n",
      "Epoch 139\n",
      "-------------------------------\n",
      "loss: 11.169759\n",
      "Test Error: \n",
      " Avg loss: 10.718965 \n",
      "\n",
      "Epoch 140\n",
      "-------------------------------\n",
      "loss: 11.065681\n",
      "Test Error: \n",
      " Avg loss: 10.722615 \n",
      "\n",
      "Epoch 141\n",
      "-------------------------------\n",
      "loss: 10.321385\n",
      "Test Error: \n",
      " Avg loss: 10.675900 \n",
      "\n",
      "Epoch 142\n",
      "-------------------------------\n",
      "loss: 10.985815\n",
      "Test Error: \n",
      " Avg loss: 10.757636 \n",
      "\n",
      "Epoch 143\n",
      "-------------------------------\n",
      "loss: 11.010807\n",
      "Test Error: \n",
      " Avg loss: 10.699948 \n",
      "\n",
      "Epoch 144\n",
      "-------------------------------\n",
      "loss: 11.250659\n",
      "Test Error: \n",
      " Avg loss: 10.918911 \n",
      "\n",
      "Epoch 145\n",
      "-------------------------------\n",
      "loss: 11.504678\n",
      "Test Error: \n",
      " Avg loss: 10.853730 \n",
      "\n",
      "Epoch 146\n",
      "-------------------------------\n",
      "loss: 10.580364\n",
      "Test Error: \n",
      " Avg loss: 10.818724 \n",
      "\n",
      "Epoch 147\n",
      "-------------------------------\n",
      "loss: 11.110930\n",
      "Test Error: \n",
      " Avg loss: 10.733246 \n",
      "\n",
      "Epoch 148\n",
      "-------------------------------\n",
      "loss: 11.221546\n",
      "Test Error: \n",
      " Avg loss: 10.715177 \n",
      "\n",
      "Epoch 149\n",
      "-------------------------------\n",
      "loss: 10.049366\n",
      "Test Error: \n",
      " Avg loss: 10.686209 \n",
      "\n",
      "Epoch 150\n",
      "-------------------------------\n",
      "loss: 11.548673\n",
      "Test Error: \n",
      " Avg loss: 10.691387 \n",
      "\n",
      "Epoch 151\n",
      "-------------------------------\n",
      "loss: 10.247832\n",
      "Test Error: \n",
      " Avg loss: 10.703857 \n",
      "\n",
      "Epoch 152\n",
      "-------------------------------\n",
      "loss: 11.062004\n",
      "Test Error: \n",
      " Avg loss: 10.793703 \n",
      "\n",
      "Epoch 153\n",
      "-------------------------------\n",
      "loss: 11.088309\n",
      "Test Error: \n",
      " Avg loss: 10.701157 \n",
      "\n",
      "Epoch 154\n",
      "-------------------------------\n",
      "loss: 10.759401\n",
      "Test Error: \n",
      " Avg loss: 10.813245 \n",
      "\n",
      "Epoch 155\n",
      "-------------------------------\n",
      "loss: 10.206046\n",
      "Test Error: \n",
      " Avg loss: 10.842275 \n",
      "\n",
      "Epoch 156\n",
      "-------------------------------\n",
      "loss: 10.658689\n",
      "Test Error: \n",
      " Avg loss: 10.684829 \n",
      "\n",
      "Epoch 157\n",
      "-------------------------------\n",
      "loss: 10.830887\n",
      "Test Error: \n",
      " Avg loss: 10.677707 \n",
      "\n",
      "Epoch 158\n",
      "-------------------------------\n",
      "loss: 10.782441\n",
      "Test Error: \n",
      " Avg loss: 10.881274 \n",
      "\n",
      "Epoch 159\n",
      "-------------------------------\n",
      "loss: 10.772779\n",
      "Test Error: \n",
      " Avg loss: 10.811939 \n",
      "\n",
      "Epoch 160\n",
      "-------------------------------\n",
      "loss: 10.162485\n",
      "Test Error: \n",
      " Avg loss: 10.751081 \n",
      "\n",
      "Epoch 161\n",
      "-------------------------------\n",
      "loss: 10.505901\n",
      "Test Error: \n",
      " Avg loss: 10.781991 \n",
      "\n",
      "Epoch 162\n",
      "-------------------------------\n",
      "loss: 10.770085\n",
      "Test Error: \n",
      " Avg loss: 10.745392 \n",
      "\n",
      "Epoch 163\n",
      "-------------------------------\n",
      "loss: 10.718347\n",
      "Test Error: \n",
      " Avg loss: 10.744432 \n",
      "\n",
      "Epoch 164\n",
      "-------------------------------\n",
      "loss: 10.058214\n",
      "Test Error: \n",
      " Avg loss: 10.841539 \n",
      "\n",
      "Epoch 165\n",
      "-------------------------------\n",
      "loss: 10.505925\n",
      "Test Error: \n",
      " Avg loss: 11.070953 \n",
      "\n",
      "Epoch 166\n",
      "-------------------------------\n",
      "loss: 10.051913\n",
      "Test Error: \n",
      " Avg loss: 10.618837 \n",
      "\n",
      "Epoch 167\n",
      "-------------------------------\n",
      "loss: 10.618291\n",
      "Test Error: \n",
      " Avg loss: 10.729061 \n",
      "\n",
      "Epoch 168\n",
      "-------------------------------\n",
      "loss: 10.147903\n",
      "Test Error: \n",
      " Avg loss: 10.943859 \n",
      "\n",
      "Epoch 169\n",
      "-------------------------------\n",
      "loss: 11.185550\n",
      "Test Error: \n",
      " Avg loss: 10.621305 \n",
      "\n",
      "Epoch 170\n",
      "-------------------------------\n",
      "loss: 10.098547\n",
      "Test Error: \n",
      " Avg loss: 10.831850 \n",
      "\n",
      "Epoch 171\n",
      "-------------------------------\n",
      "loss: 10.347529\n",
      "Test Error: \n",
      " Avg loss: 10.734001 \n",
      "\n",
      "Epoch 172\n",
      "-------------------------------\n",
      "loss: 10.872977\n",
      "Test Error: \n",
      " Avg loss: 10.548865 \n",
      "\n",
      "Epoch 173\n",
      "-------------------------------\n",
      "loss: 10.815398\n",
      "Test Error: \n",
      " Avg loss: 11.239180 \n",
      "\n",
      "Epoch 174\n",
      "-------------------------------\n",
      "loss: 11.864681\n",
      "Test Error: \n",
      " Avg loss: 10.664985 \n",
      "\n",
      "Epoch 175\n",
      "-------------------------------\n",
      "loss: 10.917820\n",
      "Test Error: \n",
      " Avg loss: 10.757867 \n",
      "\n",
      "Epoch 176\n",
      "-------------------------------\n",
      "loss: 10.883943\n",
      "Test Error: \n",
      " Avg loss: 11.103511 \n",
      "\n",
      "Epoch 177\n",
      "-------------------------------\n",
      "loss: 10.328140\n",
      "Test Error: \n",
      " Avg loss: 10.653004 \n",
      "\n",
      "Epoch 178\n",
      "-------------------------------\n",
      "loss: 10.769624\n",
      "Test Error: \n",
      " Avg loss: 10.947090 \n",
      "\n",
      "Epoch 179\n",
      "-------------------------------\n",
      "loss: 10.464158\n",
      "Test Error: \n",
      " Avg loss: 10.679494 \n",
      "\n",
      "Epoch 180\n",
      "-------------------------------\n",
      "loss: 10.615435\n",
      "Test Error: \n",
      " Avg loss: 10.958582 \n",
      "\n",
      "Epoch 181\n",
      "-------------------------------\n",
      "loss: 11.175965\n",
      "Test Error: \n",
      " Avg loss: 10.771853 \n",
      "\n",
      "Epoch 182\n",
      "-------------------------------\n",
      "loss: 10.936429\n",
      "Test Error: \n",
      " Avg loss: 10.774857 \n",
      "\n",
      "Epoch 183\n",
      "-------------------------------\n",
      "loss: 10.802771\n",
      "Test Error: \n",
      " Avg loss: 10.689317 \n",
      "\n",
      "Epoch 184\n",
      "-------------------------------\n",
      "loss: 10.989136\n",
      "Test Error: \n",
      " Avg loss: 10.838374 \n",
      "\n",
      "Epoch 185\n",
      "-------------------------------\n",
      "loss: 10.280538\n",
      "Test Error: \n",
      " Avg loss: 10.709099 \n",
      "\n",
      "Epoch 186\n",
      "-------------------------------\n",
      "loss: 10.573144\n",
      "Test Error: \n",
      " Avg loss: 10.814616 \n",
      "\n",
      "Epoch 187\n",
      "-------------------------------\n",
      "loss: 10.922399\n",
      "Test Error: \n",
      " Avg loss: 10.877658 \n",
      "\n",
      "Epoch 188\n",
      "-------------------------------\n",
      "loss: 11.619117\n",
      "Test Error: \n",
      " Avg loss: 10.675820 \n",
      "\n",
      "Epoch 189\n",
      "-------------------------------\n",
      "loss: 10.912058\n",
      "Test Error: \n",
      " Avg loss: 10.729605 \n",
      "\n",
      "Epoch 190\n",
      "-------------------------------\n",
      "loss: 10.492966\n",
      "Test Error: \n",
      " Avg loss: 10.741271 \n",
      "\n",
      "Epoch 191\n",
      "-------------------------------\n",
      "loss: 11.220552\n",
      "Test Error: \n",
      " Avg loss: 10.721416 \n",
      "\n",
      "Epoch 192\n",
      "-------------------------------\n",
      "loss: 10.416939\n",
      "Test Error: \n",
      " Avg loss: 10.699129 \n",
      "\n",
      "Epoch 193\n",
      "-------------------------------\n",
      "loss: 11.054503\n",
      "Test Error: \n",
      " Avg loss: 10.710024 \n",
      "\n",
      "Epoch 194\n",
      "-------------------------------\n",
      "loss: 10.289366\n",
      "Test Error: \n",
      " Avg loss: 10.892038 \n",
      "\n",
      "Epoch 195\n",
      "-------------------------------\n",
      "loss: 10.234077\n",
      "Test Error: \n",
      " Avg loss: 10.698734 \n",
      "\n",
      "Epoch 196\n",
      "-------------------------------\n",
      "loss: 10.199466\n",
      "Test Error: \n",
      " Avg loss: 10.708641 \n",
      "\n",
      "Epoch 197\n",
      "-------------------------------\n",
      "loss: 10.895871\n",
      "Test Error: \n",
      " Avg loss: 10.768430 \n",
      "\n",
      "Epoch 198\n",
      "-------------------------------\n",
      "loss: 11.127878\n",
      "Test Error: \n",
      " Avg loss: 10.698504 \n",
      "\n",
      "Epoch 199\n",
      "-------------------------------\n",
      "loss: 10.772942\n",
      "Test Error: \n",
      " Avg loss: 10.820974 \n",
      "\n",
      "Epoch 200\n",
      "-------------------------------\n",
      "loss: 11.207588\n",
      "Test Error: \n",
      " Avg loss: 10.704581 \n",
      "\n",
      "Epoch 201\n",
      "-------------------------------\n",
      "loss: 10.506301\n",
      "Test Error: \n",
      " Avg loss: 10.668766 \n",
      "\n",
      "Epoch 202\n",
      "-------------------------------\n",
      "loss: 10.501303\n",
      "Test Error: \n",
      " Avg loss: 10.771057 \n",
      "\n",
      "Epoch 203\n",
      "-------------------------------\n",
      "loss: 10.569312\n",
      "Test Error: \n",
      " Avg loss: 10.753425 \n",
      "\n",
      "Epoch 204\n",
      "-------------------------------\n",
      "loss: 10.895048\n",
      "Test Error: \n",
      " Avg loss: 10.719995 \n",
      "\n",
      "Epoch 205\n",
      "-------------------------------\n",
      "loss: 11.308377\n",
      "Test Error: \n",
      " Avg loss: 10.717842 \n",
      "\n",
      "Epoch 206\n",
      "-------------------------------\n",
      "loss: 10.721156\n",
      "Test Error: \n",
      " Avg loss: 10.788761 \n",
      "\n",
      "Epoch 207\n",
      "-------------------------------\n",
      "loss: 10.870241\n",
      "Test Error: \n",
      " Avg loss: 10.773756 \n",
      "\n",
      "Epoch 208\n",
      "-------------------------------\n",
      "loss: 10.835696\n",
      "Test Error: \n",
      " Avg loss: 10.708008 \n",
      "\n",
      "Epoch 209\n",
      "-------------------------------\n",
      "loss: 10.900463\n",
      "Test Error: \n",
      " Avg loss: 10.721353 \n",
      "\n",
      "Epoch 210\n",
      "-------------------------------\n",
      "loss: 10.388272\n",
      "Test Error: \n",
      " Avg loss: 10.746511 \n",
      "\n",
      "Epoch 211\n",
      "-------------------------------\n",
      "loss: 10.667051\n",
      "Test Error: \n",
      " Avg loss: 10.706649 \n",
      "\n",
      "Epoch 212\n",
      "-------------------------------\n",
      "loss: 10.979313\n",
      "Test Error: \n",
      " Avg loss: 10.675828 \n",
      "\n",
      "Epoch 213\n",
      "-------------------------------\n",
      "loss: 10.780709\n",
      "Test Error: \n",
      " Avg loss: 10.680646 \n",
      "\n",
      "Epoch 214\n",
      "-------------------------------\n",
      "loss: 10.707270\n",
      "Test Error: \n",
      " Avg loss: 10.720376 \n",
      "\n",
      "Epoch 215\n",
      "-------------------------------\n",
      "loss: 10.911086\n",
      "Test Error: \n",
      " Avg loss: 10.782285 \n",
      "\n",
      "Epoch 216\n",
      "-------------------------------\n",
      "loss: 10.972847\n",
      "Test Error: \n",
      " Avg loss: 10.728262 \n",
      "\n",
      "Epoch 217\n",
      "-------------------------------\n",
      "loss: 11.274533\n",
      "Test Error: \n",
      " Avg loss: 10.714567 \n",
      "\n",
      "Epoch 218\n",
      "-------------------------------\n",
      "loss: 10.371092\n",
      "Test Error: \n",
      " Avg loss: 10.688662 \n",
      "\n",
      "Epoch 219\n",
      "-------------------------------\n",
      "loss: 10.260291\n",
      "Test Error: \n",
      " Avg loss: 10.713296 \n",
      "\n",
      "Epoch 220\n",
      "-------------------------------\n",
      "loss: 10.435444\n",
      "Test Error: \n",
      " Avg loss: 10.758800 \n",
      "\n",
      "Epoch 221\n",
      "-------------------------------\n",
      "loss: 10.654100\n",
      "Test Error: \n",
      " Avg loss: 10.791416 \n",
      "\n",
      "Epoch 222\n",
      "-------------------------------\n",
      "loss: 10.711592\n",
      "Test Error: \n",
      " Avg loss: 10.694658 \n",
      "\n",
      "Epoch 223\n",
      "-------------------------------\n",
      "loss: 10.401501\n",
      "Test Error: \n",
      " Avg loss: 10.739114 \n",
      "\n",
      "Epoch 224\n",
      "-------------------------------\n",
      "loss: 10.502906\n",
      "Test Error: \n",
      " Avg loss: 10.726349 \n",
      "\n",
      "Epoch 225\n",
      "-------------------------------\n",
      "loss: 10.750062\n",
      "Test Error: \n",
      " Avg loss: 10.727106 \n",
      "\n",
      "Epoch 226\n",
      "-------------------------------\n",
      "loss: 11.001585\n",
      "Test Error: \n",
      " Avg loss: 10.699633 \n",
      "\n",
      "Epoch 227\n",
      "-------------------------------\n",
      "loss: 10.736137\n",
      "Test Error: \n",
      " Avg loss: 10.719297 \n",
      "\n",
      "Epoch 228\n",
      "-------------------------------\n",
      "loss: 10.922527\n",
      "Test Error: \n",
      " Avg loss: 10.741502 \n",
      "\n",
      "Epoch 229\n",
      "-------------------------------\n",
      "loss: 10.732398\n",
      "Test Error: \n",
      " Avg loss: 10.728009 \n",
      "\n",
      "Epoch 230\n",
      "-------------------------------\n",
      "loss: 10.161252\n",
      "Test Error: \n",
      " Avg loss: 10.871645 \n",
      "\n",
      "Epoch 231\n",
      "-------------------------------\n",
      "loss: 11.169342\n",
      "Test Error: \n",
      " Avg loss: 10.686638 \n",
      "\n",
      "Epoch 232\n",
      "-------------------------------\n",
      "loss: 10.498311\n",
      "Test Error: \n",
      " Avg loss: 10.713898 \n",
      "\n",
      "Epoch 233\n",
      "-------------------------------\n",
      "loss: 11.022115\n",
      "Test Error: \n",
      " Avg loss: 10.732742 \n",
      "\n",
      "Epoch 234\n",
      "-------------------------------\n",
      "loss: 10.781505\n",
      "Test Error: \n",
      " Avg loss: 10.723329 \n",
      "\n",
      "Epoch 235\n",
      "-------------------------------\n",
      "loss: 10.734913\n",
      "Test Error: \n",
      " Avg loss: 10.665563 \n",
      "\n",
      "Epoch 236\n",
      "-------------------------------\n",
      "loss: 11.575559\n",
      "Test Error: \n",
      " Avg loss: 10.711272 \n",
      "\n",
      "Epoch 237\n",
      "-------------------------------\n",
      "loss: 10.500826\n",
      "Test Error: \n",
      " Avg loss: 10.661347 \n",
      "\n",
      "Epoch 238\n",
      "-------------------------------\n",
      "loss: 10.186462\n",
      "Test Error: \n",
      " Avg loss: 10.718334 \n",
      "\n",
      "Epoch 239\n",
      "-------------------------------\n",
      "loss: 11.004358\n",
      "Test Error: \n",
      " Avg loss: 10.689030 \n",
      "\n",
      "Epoch 240\n",
      "-------------------------------\n",
      "loss: 10.407436\n",
      "Test Error: \n",
      " Avg loss: 10.708531 \n",
      "\n",
      "Epoch 241\n",
      "-------------------------------\n",
      "loss: 10.757544\n",
      "Test Error: \n",
      " Avg loss: 10.685207 \n",
      "\n",
      "Epoch 242\n",
      "-------------------------------\n",
      "loss: 10.336403\n",
      "Test Error: \n",
      " Avg loss: 10.735678 \n",
      "\n",
      "Epoch 243\n",
      "-------------------------------\n",
      "loss: 10.195443\n",
      "Test Error: \n",
      " Avg loss: 10.692689 \n",
      "\n",
      "Epoch 244\n",
      "-------------------------------\n",
      "loss: 10.659952\n",
      "Test Error: \n",
      " Avg loss: 10.667993 \n",
      "\n",
      "Epoch 245\n",
      "-------------------------------\n",
      "loss: 10.371634\n",
      "Test Error: \n",
      " Avg loss: 10.771833 \n",
      "\n",
      "Epoch 246\n",
      "-------------------------------\n",
      "loss: 10.473923\n",
      "Test Error: \n",
      " Avg loss: 10.808255 \n",
      "\n",
      "Epoch 247\n",
      "-------------------------------\n",
      "loss: 10.849729\n",
      "Test Error: \n",
      " Avg loss: 10.694439 \n",
      "\n",
      "Epoch 248\n",
      "-------------------------------\n",
      "loss: 11.252748\n",
      "Test Error: \n",
      " Avg loss: 10.806639 \n",
      "\n",
      "Epoch 249\n",
      "-------------------------------\n",
      "loss: 10.872040\n",
      "Test Error: \n",
      " Avg loss: 10.726103 \n",
      "\n",
      "Epoch 250\n",
      "-------------------------------\n",
      "loss: 10.967666\n",
      "Test Error: \n",
      " Avg loss: 10.747299 \n",
      "\n",
      "Epoch 251\n",
      "-------------------------------\n",
      "loss: 10.413370\n",
      "Test Error: \n",
      " Avg loss: 10.708908 \n",
      "\n",
      "Epoch 252\n",
      "-------------------------------\n",
      "loss: 10.967802\n",
      "Test Error: \n",
      " Avg loss: 10.767477 \n",
      "\n",
      "Epoch 253\n",
      "-------------------------------\n",
      "loss: 10.904339\n",
      "Test Error: \n",
      " Avg loss: 10.964699 \n",
      "\n",
      "Epoch 254\n",
      "-------------------------------\n",
      "loss: 11.051483\n",
      "Test Error: \n",
      " Avg loss: 10.677172 \n",
      "\n",
      "Epoch 255\n",
      "-------------------------------\n",
      "loss: 10.428645\n",
      "Test Error: \n",
      " Avg loss: 10.698232 \n",
      "\n",
      "Epoch 256\n",
      "-------------------------------\n",
      "loss: 10.001193\n",
      "Test Error: \n",
      " Avg loss: 10.711886 \n",
      "\n",
      "Epoch 257\n",
      "-------------------------------\n",
      "loss: 10.470975\n",
      "Test Error: \n",
      " Avg loss: 10.694696 \n",
      "\n",
      "Epoch 258\n",
      "-------------------------------\n",
      "loss: 11.142784\n",
      "Test Error: \n",
      " Avg loss: 10.679970 \n",
      "\n",
      "Epoch 259\n",
      "-------------------------------\n",
      "loss: 11.101334\n",
      "Test Error: \n",
      " Avg loss: 10.698148 \n",
      "\n",
      "Epoch 260\n",
      "-------------------------------\n",
      "loss: 10.685400\n",
      "Test Error: \n",
      " Avg loss: 10.730500 \n",
      "\n",
      "Epoch 261\n",
      "-------------------------------\n",
      "loss: 11.002850\n",
      "Test Error: \n",
      " Avg loss: 10.727940 \n",
      "\n",
      "Epoch 262\n",
      "-------------------------------\n",
      "loss: 10.541781\n",
      "Test Error: \n",
      " Avg loss: 10.777788 \n",
      "\n",
      "Epoch 263\n",
      "-------------------------------\n",
      "loss: 10.377346\n",
      "Test Error: \n",
      " Avg loss: 10.703069 \n",
      "\n",
      "Epoch 264\n",
      "-------------------------------\n",
      "loss: 10.771127\n",
      "Test Error: \n",
      " Avg loss: 10.741752 \n",
      "\n",
      "Epoch 265\n",
      "-------------------------------\n",
      "loss: 10.616667\n",
      "Test Error: \n",
      " Avg loss: 10.712941 \n",
      "\n",
      "Epoch 266\n",
      "-------------------------------\n",
      "loss: 10.300147\n",
      "Test Error: \n",
      " Avg loss: 10.706713 \n",
      "\n",
      "Epoch 267\n",
      "-------------------------------\n",
      "loss: 10.769024\n",
      "Test Error: \n",
      " Avg loss: 10.727757 \n",
      "\n",
      "Epoch 268\n",
      "-------------------------------\n",
      "loss: 10.697509\n",
      "Test Error: \n",
      " Avg loss: 10.696015 \n",
      "\n",
      "Epoch 269\n",
      "-------------------------------\n",
      "loss: 10.653582\n",
      "Test Error: \n",
      " Avg loss: 10.713750 \n",
      "\n",
      "Epoch 270\n",
      "-------------------------------\n",
      "loss: 10.757240\n",
      "Test Error: \n",
      " Avg loss: 10.706048 \n",
      "\n",
      "Epoch 271\n",
      "-------------------------------\n",
      "loss: 9.938544\n",
      "Test Error: \n",
      " Avg loss: 10.671789 \n",
      "\n",
      "Epoch 272\n",
      "-------------------------------\n",
      "loss: 10.428404\n",
      "Test Error: \n",
      " Avg loss: 10.743033 \n",
      "\n",
      "Epoch 273\n",
      "-------------------------------\n",
      "loss: 10.684233\n",
      "Test Error: \n",
      " Avg loss: 10.669975 \n",
      "\n",
      "Epoch 274\n",
      "-------------------------------\n",
      "loss: 10.179258\n",
      "Test Error: \n",
      " Avg loss: 10.762192 \n",
      "\n",
      "Epoch 275\n",
      "-------------------------------\n",
      "loss: 10.270513\n",
      "Test Error: \n",
      " Avg loss: 10.715714 \n",
      "\n",
      "Epoch 276\n",
      "-------------------------------\n",
      "loss: 11.412510\n",
      "Test Error: \n",
      " Avg loss: 10.678806 \n",
      "\n",
      "Epoch 277\n",
      "-------------------------------\n",
      "loss: 10.699774\n",
      "Test Error: \n",
      " Avg loss: 10.709093 \n",
      "\n",
      "Epoch 278\n",
      "-------------------------------\n",
      "loss: 10.975845\n",
      "Test Error: \n",
      " Avg loss: 10.766225 \n",
      "\n",
      "Epoch 279\n",
      "-------------------------------\n",
      "loss: 11.084485\n",
      "Test Error: \n",
      " Avg loss: 10.741636 \n",
      "\n",
      "Epoch 280\n",
      "-------------------------------\n",
      "loss: 10.770680\n",
      "Test Error: \n",
      " Avg loss: 10.737680 \n",
      "\n",
      "Epoch 281\n",
      "-------------------------------\n",
      "loss: 10.586531\n",
      "Test Error: \n",
      " Avg loss: 10.730574 \n",
      "\n",
      "Epoch 282\n",
      "-------------------------------\n",
      "loss: 10.497978\n",
      "Test Error: \n",
      " Avg loss: 10.720117 \n",
      "\n",
      "Epoch 283\n",
      "-------------------------------\n",
      "loss: 10.810238\n",
      "Test Error: \n",
      " Avg loss: 10.704568 \n",
      "\n",
      "Epoch 284\n",
      "-------------------------------\n",
      "loss: 10.127669\n",
      "Test Error: \n",
      " Avg loss: 10.705846 \n",
      "\n",
      "Epoch 285\n",
      "-------------------------------\n",
      "loss: 10.358794\n",
      "Test Error: \n",
      " Avg loss: 10.699166 \n",
      "\n",
      "Epoch 286\n",
      "-------------------------------\n",
      "loss: 10.619302\n",
      "Test Error: \n",
      " Avg loss: 10.684871 \n",
      "\n",
      "Epoch 287\n",
      "-------------------------------\n",
      "loss: 10.533665\n",
      "Test Error: \n",
      " Avg loss: 10.702699 \n",
      "\n",
      "Epoch 288\n",
      "-------------------------------\n",
      "loss: 10.612516\n",
      "Test Error: \n",
      " Avg loss: 10.694507 \n",
      "\n",
      "Epoch 289\n",
      "-------------------------------\n",
      "loss: 10.415270\n",
      "Test Error: \n",
      " Avg loss: 10.663364 \n",
      "\n",
      "Epoch 290\n",
      "-------------------------------\n",
      "loss: 10.354758\n",
      "Test Error: \n",
      " Avg loss: 10.689096 \n",
      "\n",
      "Epoch 291\n",
      "-------------------------------\n",
      "loss: 10.182961\n",
      "Test Error: \n",
      " Avg loss: 10.728232 \n",
      "\n",
      "Epoch 292\n",
      "-------------------------------\n",
      "loss: 10.359322\n",
      "Test Error: \n",
      " Avg loss: 10.719022 \n",
      "\n",
      "Epoch 293\n",
      "-------------------------------\n",
      "loss: 10.433217\n",
      "Test Error: \n",
      " Avg loss: 10.676479 \n",
      "\n",
      "Epoch 294\n",
      "-------------------------------\n",
      "loss: 10.582652\n",
      "Test Error: \n",
      " Avg loss: 10.734909 \n",
      "\n",
      "Epoch 295\n",
      "-------------------------------\n",
      "loss: 11.044045\n",
      "Test Error: \n",
      " Avg loss: 10.700713 \n",
      "\n",
      "Epoch 296\n",
      "-------------------------------\n",
      "loss: 10.836222\n",
      "Test Error: \n",
      " Avg loss: 10.716137 \n",
      "\n",
      "Epoch 297\n",
      "-------------------------------\n",
      "loss: 10.009750\n",
      "Test Error: \n",
      " Avg loss: 10.721543 \n",
      "\n",
      "Epoch 298\n",
      "-------------------------------\n",
      "loss: 10.633933\n",
      "Test Error: \n",
      " Avg loss: 10.695329 \n",
      "\n",
      "Epoch 299\n",
      "-------------------------------\n",
      "loss: 10.773948\n",
      "Test Error: \n",
      " Avg loss: 10.694793 \n",
      "\n",
      "Epoch 300\n",
      "-------------------------------\n",
      "loss: 10.586692\n",
      "Test Error: \n",
      " Avg loss: 10.708439 \n",
      "\n",
      "Epoch 301\n",
      "-------------------------------\n",
      "loss: 10.457475\n",
      "Test Error: \n",
      " Avg loss: 10.740091 \n",
      "\n",
      "Epoch 302\n",
      "-------------------------------\n",
      "loss: 11.180326\n",
      "Test Error: \n",
      " Avg loss: 10.744823 \n",
      "\n",
      "Epoch 303\n",
      "-------------------------------\n",
      "loss: 10.628520\n",
      "Test Error: \n",
      " Avg loss: 10.658025 \n",
      "\n",
      "Epoch 304\n",
      "-------------------------------\n",
      "loss: 11.007407\n",
      "Test Error: \n",
      " Avg loss: 10.685538 \n",
      "\n",
      "Epoch 305\n",
      "-------------------------------\n",
      "loss: 10.802409\n",
      "Test Error: \n",
      " Avg loss: 10.737783 \n",
      "\n",
      "Epoch 306\n",
      "-------------------------------\n",
      "loss: 10.478476\n",
      "Test Error: \n",
      " Avg loss: 10.687904 \n",
      "\n",
      "Epoch 307\n",
      "-------------------------------\n",
      "loss: 10.405681\n",
      "Test Error: \n",
      " Avg loss: 10.710894 \n",
      "\n",
      "Epoch 308\n",
      "-------------------------------\n",
      "loss: 10.380886\n",
      "Test Error: \n",
      " Avg loss: 10.734701 \n",
      "\n",
      "Epoch 309\n",
      "-------------------------------\n",
      "loss: 10.556389\n",
      "Test Error: \n",
      " Avg loss: 10.687820 \n",
      "\n",
      "Epoch 310\n",
      "-------------------------------\n",
      "loss: 10.919666\n",
      "Test Error: \n",
      " Avg loss: 10.687831 \n",
      "\n",
      "Epoch 311\n",
      "-------------------------------\n",
      "loss: 11.028394\n",
      "Test Error: \n",
      " Avg loss: 10.705669 \n",
      "\n",
      "Epoch 312\n",
      "-------------------------------\n",
      "loss: 10.759908\n",
      "Test Error: \n",
      " Avg loss: 10.702442 \n",
      "\n",
      "Epoch 313\n",
      "-------------------------------\n",
      "loss: 10.742420\n",
      "Test Error: \n",
      " Avg loss: 10.732724 \n",
      "\n",
      "Epoch 314\n",
      "-------------------------------\n",
      "loss: 10.633196\n",
      "Test Error: \n",
      " Avg loss: 10.733779 \n",
      "\n",
      "Epoch 315\n",
      "-------------------------------\n",
      "loss: 10.625609\n",
      "Test Error: \n",
      " Avg loss: 10.696306 \n",
      "\n",
      "Epoch 316\n",
      "-------------------------------\n",
      "loss: 10.802553\n",
      "Test Error: \n",
      " Avg loss: 10.698595 \n",
      "\n",
      "Epoch 317\n",
      "-------------------------------\n",
      "loss: 10.346520\n",
      "Test Error: \n",
      " Avg loss: 10.703913 \n",
      "\n",
      "Epoch 318\n",
      "-------------------------------\n",
      "loss: 10.461599\n",
      "Test Error: \n",
      " Avg loss: 10.745375 \n",
      "\n",
      "Epoch 319\n",
      "-------------------------------\n",
      "loss: 10.716742\n",
      "Test Error: \n",
      " Avg loss: 10.715227 \n",
      "\n",
      "Epoch 320\n",
      "-------------------------------\n",
      "loss: 10.086798\n",
      "Test Error: \n",
      " Avg loss: 10.743156 \n",
      "\n",
      "Epoch 321\n",
      "-------------------------------\n",
      "loss: 10.204944\n",
      "Test Error: \n",
      " Avg loss: 10.670842 \n",
      "\n",
      "Epoch 322\n",
      "-------------------------------\n",
      "loss: 10.421569\n",
      "Test Error: \n",
      " Avg loss: 10.681469 \n",
      "\n",
      "Epoch 323\n",
      "-------------------------------\n",
      "loss: 10.224971\n",
      "Test Error: \n",
      " Avg loss: 10.776093 \n",
      "\n",
      "Epoch 324\n",
      "-------------------------------\n",
      "loss: 11.494317\n",
      "Test Error: \n",
      " Avg loss: 10.730318 \n",
      "\n",
      "Epoch 325\n",
      "-------------------------------\n",
      "loss: 10.483708\n",
      "Test Error: \n",
      " Avg loss: 10.688247 \n",
      "\n",
      "Epoch 326\n",
      "-------------------------------\n",
      "loss: 10.096918\n",
      "Test Error: \n",
      " Avg loss: 10.680072 \n",
      "\n",
      "Epoch 327\n",
      "-------------------------------\n",
      "loss: 10.359408\n",
      "Test Error: \n",
      " Avg loss: 10.673978 \n",
      "\n",
      "Epoch 328\n",
      "-------------------------------\n",
      "loss: 10.857109\n",
      "Test Error: \n",
      " Avg loss: 10.717536 \n",
      "\n",
      "Epoch 329\n",
      "-------------------------------\n",
      "loss: 11.411684\n",
      "Test Error: \n",
      " Avg loss: 10.737268 \n",
      "\n",
      "Epoch 330\n",
      "-------------------------------\n",
      "loss: 10.097033\n",
      "Test Error: \n",
      " Avg loss: 10.691552 \n",
      "\n",
      "Epoch 331\n",
      "-------------------------------\n",
      "loss: 10.390959\n",
      "Test Error: \n",
      " Avg loss: 10.709056 \n",
      "\n",
      "Epoch 332\n",
      "-------------------------------\n",
      "loss: 9.639156\n",
      "Test Error: \n",
      " Avg loss: 10.683756 \n",
      "\n",
      "Epoch 333\n",
      "-------------------------------\n",
      "loss: 10.312703\n",
      "Test Error: \n",
      " Avg loss: 10.695808 \n",
      "\n",
      "Epoch 334\n",
      "-------------------------------\n",
      "loss: 10.814236\n",
      "Test Error: \n",
      " Avg loss: 10.722091 \n",
      "\n",
      "Epoch 335\n",
      "-------------------------------\n",
      "loss: 10.510691\n",
      "Test Error: \n",
      " Avg loss: 10.740508 \n",
      "\n",
      "Epoch 336\n",
      "-------------------------------\n",
      "loss: 10.428494\n",
      "Test Error: \n",
      " Avg loss: 10.678227 \n",
      "\n",
      "Epoch 337\n",
      "-------------------------------\n",
      "loss: 10.920780\n",
      "Test Error: \n",
      " Avg loss: 10.722667 \n",
      "\n",
      "Epoch 338\n",
      "-------------------------------\n",
      "loss: 10.985953\n",
      "Test Error: \n",
      " Avg loss: 10.720716 \n",
      "\n",
      "Epoch 339\n",
      "-------------------------------\n",
      "loss: 10.403458\n",
      "Test Error: \n",
      " Avg loss: 10.722283 \n",
      "\n",
      "Epoch 340\n",
      "-------------------------------\n",
      "loss: 11.039041\n",
      "Test Error: \n",
      " Avg loss: 10.700602 \n",
      "\n",
      "Epoch 341\n",
      "-------------------------------\n",
      "loss: 10.515343\n",
      "Test Error: \n",
      " Avg loss: 10.699160 \n",
      "\n",
      "Epoch 342\n",
      "-------------------------------\n",
      "loss: 10.636815\n",
      "Test Error: \n",
      " Avg loss: 10.705514 \n",
      "\n",
      "Epoch 343\n",
      "-------------------------------\n",
      "loss: 10.322457\n",
      "Test Error: \n",
      " Avg loss: 10.696335 \n",
      "\n",
      "Epoch 344\n",
      "-------------------------------\n",
      "loss: 10.410360\n",
      "Test Error: \n",
      " Avg loss: 10.739728 \n",
      "\n",
      "Epoch 345\n",
      "-------------------------------\n",
      "loss: 10.887246\n",
      "Test Error: \n",
      " Avg loss: 10.707351 \n",
      "\n",
      "Epoch 346\n",
      "-------------------------------\n",
      "loss: 10.809265\n",
      "Test Error: \n",
      " Avg loss: 10.709996 \n",
      "\n",
      "Epoch 347\n",
      "-------------------------------\n",
      "loss: 10.349193\n",
      "Test Error: \n",
      " Avg loss: 10.675674 \n",
      "\n",
      "Epoch 348\n",
      "-------------------------------\n",
      "loss: 10.495870\n",
      "Test Error: \n",
      " Avg loss: 10.690964 \n",
      "\n",
      "Epoch 349\n",
      "-------------------------------\n",
      "loss: 10.605650\n",
      "Test Error: \n",
      " Avg loss: 10.679484 \n",
      "\n",
      "Epoch 350\n",
      "-------------------------------\n",
      "loss: 10.370816\n",
      "Test Error: \n",
      " Avg loss: 10.746411 \n",
      "\n",
      "Epoch 351\n",
      "-------------------------------\n",
      "loss: 11.033075\n",
      "Test Error: \n",
      " Avg loss: 10.711547 \n",
      "\n",
      "Epoch 352\n",
      "-------------------------------\n",
      "loss: 11.159345\n",
      "Test Error: \n",
      " Avg loss: 10.680343 \n",
      "\n",
      "Epoch 353\n",
      "-------------------------------\n",
      "loss: 10.907801\n",
      "Test Error: \n",
      " Avg loss: 10.679776 \n",
      "\n",
      "Epoch 354\n",
      "-------------------------------\n",
      "loss: 10.603568\n",
      "Test Error: \n",
      " Avg loss: 10.706520 \n",
      "\n",
      "Epoch 355\n",
      "-------------------------------\n",
      "loss: 10.391858\n",
      "Test Error: \n",
      " Avg loss: 10.728921 \n",
      "\n",
      "Epoch 356\n",
      "-------------------------------\n",
      "loss: 10.150957\n",
      "Test Error: \n",
      " Avg loss: 10.682226 \n",
      "\n",
      "Epoch 357\n",
      "-------------------------------\n",
      "loss: 10.391737\n",
      "Test Error: \n",
      " Avg loss: 10.710526 \n",
      "\n",
      "Epoch 358\n",
      "-------------------------------\n",
      "loss: 10.018215\n",
      "Test Error: \n",
      " Avg loss: 10.711856 \n",
      "\n",
      "Epoch 359\n",
      "-------------------------------\n",
      "loss: 10.343257\n",
      "Test Error: \n",
      " Avg loss: 10.704367 \n",
      "\n",
      "Epoch 360\n",
      "-------------------------------\n",
      "loss: 10.403341\n",
      "Test Error: \n",
      " Avg loss: 10.705369 \n",
      "\n",
      "Epoch 361\n",
      "-------------------------------\n",
      "loss: 10.261151\n",
      "Test Error: \n",
      " Avg loss: 10.706067 \n",
      "\n",
      "Epoch 362\n",
      "-------------------------------\n",
      "loss: 11.052795\n",
      "Test Error: \n",
      " Avg loss: 10.714486 \n",
      "\n",
      "Epoch 363\n",
      "-------------------------------\n",
      "loss: 10.947233\n",
      "Test Error: \n",
      " Avg loss: 10.714003 \n",
      "\n",
      "Epoch 364\n",
      "-------------------------------\n",
      "loss: 10.509388\n",
      "Test Error: \n",
      " Avg loss: 10.673562 \n",
      "\n",
      "Epoch 365\n",
      "-------------------------------\n",
      "loss: 11.205849\n",
      "Test Error: \n",
      " Avg loss: 10.656237 \n",
      "\n",
      "Epoch 366\n",
      "-------------------------------\n",
      "loss: 10.832367\n",
      "Test Error: \n",
      " Avg loss: 10.664160 \n",
      "\n",
      "Epoch 367\n",
      "-------------------------------\n",
      "loss: 11.144343\n",
      "Test Error: \n",
      " Avg loss: 10.684956 \n",
      "\n",
      "Epoch 368\n",
      "-------------------------------\n",
      "loss: 10.652611\n",
      "Test Error: \n",
      " Avg loss: 10.761332 \n",
      "\n",
      "Epoch 369\n",
      "-------------------------------\n",
      "loss: 10.771238\n",
      "Test Error: \n",
      " Avg loss: 10.686005 \n",
      "\n",
      "Epoch 370\n",
      "-------------------------------\n",
      "loss: 10.557522\n",
      "Test Error: \n",
      " Avg loss: 10.699502 \n",
      "\n",
      "Epoch 371\n",
      "-------------------------------\n",
      "loss: 10.492825\n",
      "Test Error: \n",
      " Avg loss: 10.735076 \n",
      "\n",
      "Epoch 372\n",
      "-------------------------------\n",
      "loss: 10.619670\n",
      "Test Error: \n",
      " Avg loss: 10.670806 \n",
      "\n",
      "Epoch 373\n",
      "-------------------------------\n",
      "loss: 10.612166\n",
      "Test Error: \n",
      " Avg loss: 10.726557 \n",
      "\n",
      "Epoch 374\n",
      "-------------------------------\n",
      "loss: 10.894819\n",
      "Test Error: \n",
      " Avg loss: 10.748552 \n",
      "\n",
      "Epoch 375\n",
      "-------------------------------\n",
      "loss: 10.456603\n",
      "Test Error: \n",
      " Avg loss: 10.704238 \n",
      "\n",
      "Epoch 376\n",
      "-------------------------------\n",
      "loss: 10.952856\n",
      "Test Error: \n",
      " Avg loss: 10.674062 \n",
      "\n",
      "Epoch 377\n",
      "-------------------------------\n",
      "loss: 10.444117\n",
      "Test Error: \n",
      " Avg loss: 10.706770 \n",
      "\n",
      "Epoch 378\n",
      "-------------------------------\n",
      "loss: 10.799260\n",
      "Test Error: \n",
      " Avg loss: 10.718949 \n",
      "\n",
      "Epoch 379\n",
      "-------------------------------\n",
      "loss: 10.851308\n",
      "Test Error: \n",
      " Avg loss: 10.704071 \n",
      "\n",
      "Epoch 380\n",
      "-------------------------------\n",
      "loss: 10.949989\n",
      "Test Error: \n",
      " Avg loss: 10.732142 \n",
      "\n",
      "Epoch 381\n",
      "-------------------------------\n",
      "loss: 10.421000\n",
      "Test Error: \n",
      " Avg loss: 10.723193 \n",
      "\n",
      "Epoch 382\n",
      "-------------------------------\n",
      "loss: 10.919563\n",
      "Test Error: \n",
      " Avg loss: 10.712419 \n",
      "\n",
      "Epoch 383\n",
      "-------------------------------\n",
      "loss: 11.102039\n",
      "Test Error: \n",
      " Avg loss: 10.738946 \n",
      "\n",
      "Epoch 384\n",
      "-------------------------------\n",
      "loss: 11.026421\n",
      "Test Error: \n",
      " Avg loss: 10.663657 \n",
      "\n",
      "Epoch 385\n",
      "-------------------------------\n",
      "loss: 10.408799\n",
      "Test Error: \n",
      " Avg loss: 10.682721 \n",
      "\n",
      "Epoch 386\n",
      "-------------------------------\n",
      "loss: 10.568846\n",
      "Test Error: \n",
      " Avg loss: 10.794270 \n",
      "\n",
      "Epoch 387\n",
      "-------------------------------\n",
      "loss: 10.936803\n",
      "Test Error: \n",
      " Avg loss: 10.722942 \n",
      "\n",
      "Epoch 388\n",
      "-------------------------------\n",
      "loss: 11.044988\n",
      "Test Error: \n",
      " Avg loss: 10.677723 \n",
      "\n",
      "Epoch 389\n",
      "-------------------------------\n",
      "loss: 10.714832\n",
      "Test Error: \n",
      " Avg loss: 10.698732 \n",
      "\n",
      "Epoch 390\n",
      "-------------------------------\n",
      "loss: 11.066088\n",
      "Test Error: \n",
      " Avg loss: 10.734445 \n",
      "\n",
      "Epoch 391\n",
      "-------------------------------\n",
      "loss: 11.106538\n",
      "Test Error: \n",
      " Avg loss: 10.702000 \n",
      "\n",
      "Epoch 392\n",
      "-------------------------------\n",
      "loss: 10.783268\n",
      "Test Error: \n",
      " Avg loss: 10.657432 \n",
      "\n",
      "Epoch 393\n",
      "-------------------------------\n",
      "loss: 10.367741\n",
      "Test Error: \n",
      " Avg loss: 10.726759 \n",
      "\n",
      "Epoch 394\n",
      "-------------------------------\n",
      "loss: 10.455942\n",
      "Test Error: \n",
      " Avg loss: 10.727915 \n",
      "\n",
      "Epoch 395\n",
      "-------------------------------\n",
      "loss: 11.411529\n",
      "Test Error: \n",
      " Avg loss: 10.709039 \n",
      "\n",
      "Epoch 396\n",
      "-------------------------------\n",
      "loss: 10.652374\n",
      "Test Error: \n",
      " Avg loss: 10.721012 \n",
      "\n",
      "Epoch 397\n",
      "-------------------------------\n",
      "loss: 10.474628\n",
      "Test Error: \n",
      " Avg loss: 10.726740 \n",
      "\n",
      "Epoch 398\n",
      "-------------------------------\n",
      "loss: 10.481401\n",
      "Test Error: \n",
      " Avg loss: 10.685354 \n",
      "\n",
      "Epoch 399\n",
      "-------------------------------\n",
      "loss: 10.370145\n",
      "Test Error: \n",
      " Avg loss: 10.679051 \n",
      "\n",
      "Epoch 400\n",
      "-------------------------------\n",
      "loss: 10.321975\n",
      "Test Error: \n",
      " Avg loss: 10.705352 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.L1Loss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.05)\n",
    "step_size = 4*len(dataset_train)\n",
    "clr = cyclical_lr(step_size)\n",
    "# scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, [clr])\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(loader_train, model, loss_fn, optimizer)\n",
    "    test_loop(loader_test, model, loss_fn)\n",
    "    # optimizer.lr = exp_decay(t)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "id": "24fd8873",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-20T12:42:09.182058Z",
     "iopub.status.busy": "2022-02-20T12:42:09.181532Z",
     "iopub.status.idle": "2022-02-20T12:42:09.185227Z",
     "shell.execute_reply": "2022-02-20T12:42:09.184790Z",
     "shell.execute_reply.started": "2022-02-20T12:40:33.242652Z"
    },
    "papermill": {
     "duration": 0.057446,
     "end_time": "2022-02-20T12:42:09.185364",
     "exception": false,
     "start_time": "2022-02-20T12:42:09.127918",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def SMAPE(y_true, y_pred):\n",
    "    denominator = (y_true + np.abs(y_pred)) / 200.0\n",
    "    diff = np.abs(y_true - y_pred) / denominator\n",
    "    diff[denominator == 0] = 0.0\n",
    "    return np.mean(diff)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "id": "0cec1cbf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-20T12:42:09.288885Z",
     "iopub.status.busy": "2022-02-20T12:42:09.288110Z",
     "iopub.status.idle": "2022-02-20T12:42:11.672675Z",
     "shell.execute_reply": "2022-02-20T12:42:11.672157Z",
     "shell.execute_reply.started": "2022-02-20T12:40:33.809849Z"
    },
    "papermill": {
     "duration": 2.439157,
     "end_time": "2022-02-20T12:42:11.672805",
     "exception": false,
     "start_time": "2022-02-20T12:42:09.233648",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_score  41.304356 test_score 41.177177\n"
     ]
    }
   ],
   "source": [
    "x_train = x_train.to(device)\n",
    "x_test = x_test.to(device)\n",
    "\n",
    "train_pred = model(x_train)\n",
    "test_pred = model(x_test)\n",
    "\n",
    "train_score = SMAPE(y_train.cpu().detach().numpy(), train_pred.cpu().detach().numpy())\n",
    "test_score = SMAPE(y_test.cpu().detach().numpy(), test_pred.cpu().detach().numpy())\n",
    "\n",
    "print('train_score ', train_score, 'test_score', test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "id": "f824c94f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-20T12:42:11.774179Z",
     "iopub.status.busy": "2022-02-20T12:42:11.773656Z",
     "iopub.status.idle": "2022-02-20T12:42:11.777283Z",
     "shell.execute_reply": "2022-02-20T12:42:11.776872Z",
     "shell.execute_reply.started": "2022-02-20T12:40:56.356572Z"
    },
    "papermill": {
     "duration": 0.055712,
     "end_time": "2022-02-20T12:42:11.777397",
     "exception": false,
     "start_time": "2022-02-20T12:42:11.721685",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test = test.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "id": "d712be2c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-20T12:42:11.879463Z",
     "iopub.status.busy": "2022-02-20T12:42:11.878870Z",
     "iopub.status.idle": "2022-02-20T12:42:11.881198Z",
     "shell.execute_reply": "2022-02-20T12:42:11.881668Z",
     "shell.execute_reply.started": "2022-02-20T12:40:56.934746Z"
    },
    "papermill": {
     "duration": 0.055685,
     "end_time": "2022-02-20T12:42:11.881788",
     "exception": false,
     "start_time": "2022-02-20T12:42:11.826103",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_pred = model(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "id": "bbe4fcad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[296.2619],\n",
       "        [250.7177],\n",
       "        [249.0258],\n",
       "        ...,\n",
       "        [249.0495],\n",
       "        [277.7478],\n",
       "        [252.3047]], grad_fn=<ToCopyBackward0>)"
      ]
     },
     "execution_count": 487,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pred.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "id": "1562e56d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-20T12:42:11.985255Z",
     "iopub.status.busy": "2022-02-20T12:42:11.984585Z",
     "iopub.status.idle": "2022-02-20T12:42:12.010174Z",
     "shell.execute_reply": "2022-02-20T12:42:12.009745Z",
     "shell.execute_reply.started": "2022-02-20T12:40:57.882610Z"
    },
    "papermill": {
     "duration": 0.078128,
     "end_time": "2022-02-20T12:42:12.010305",
     "exception": false,
     "start_time": "2022-02-20T12:42:11.932177",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "unique, counts = np.unique(y_train.cpu().detach().numpy(), return_counts=True)\n",
    "unique_p, counts_p = np.unique(np.round(train_pred.cpu().detach().numpy()), return_counts=True)\n",
    "d_p = pd.DataFrame()\n",
    "d_p.index = unique_p\n",
    "d_p['count_pred'] = counts_p\n",
    "d = pd.DataFrame()\n",
    "d.index = unique\n",
    "d['count_true'] = counts\n",
    "d1 = d.join(d_p)\n",
    "d1 = d1.fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "id": "3ed2fe28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>249.0</th>\n",
       "      <td>4240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250.0</th>\n",
       "      <td>647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251.0</th>\n",
       "      <td>483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252.0</th>\n",
       "      <td>359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253.0</th>\n",
       "      <td>405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254.0</th>\n",
       "      <td>233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255.0</th>\n",
       "      <td>226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256.0</th>\n",
       "      <td>216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257.0</th>\n",
       "      <td>233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258.0</th>\n",
       "      <td>236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259.0</th>\n",
       "      <td>235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260.0</th>\n",
       "      <td>245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261.0</th>\n",
       "      <td>226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262.0</th>\n",
       "      <td>262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263.0</th>\n",
       "      <td>263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264.0</th>\n",
       "      <td>231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265.0</th>\n",
       "      <td>174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>266.0</th>\n",
       "      <td>230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267.0</th>\n",
       "      <td>190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268.0</th>\n",
       "      <td>209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269.0</th>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270.0</th>\n",
       "      <td>185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271.0</th>\n",
       "      <td>209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>272.0</th>\n",
       "      <td>222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>273.0</th>\n",
       "      <td>282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274.0</th>\n",
       "      <td>213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275.0</th>\n",
       "      <td>203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>276.0</th>\n",
       "      <td>218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277.0</th>\n",
       "      <td>249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278.0</th>\n",
       "      <td>223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279.0</th>\n",
       "      <td>207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280.0</th>\n",
       "      <td>207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281.0</th>\n",
       "      <td>208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282.0</th>\n",
       "      <td>201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>283.0</th>\n",
       "      <td>281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284.0</th>\n",
       "      <td>202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285.0</th>\n",
       "      <td>206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>286.0</th>\n",
       "      <td>192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>287.0</th>\n",
       "      <td>208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288.0</th>\n",
       "      <td>202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289.0</th>\n",
       "      <td>180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290.0</th>\n",
       "      <td>212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291.0</th>\n",
       "      <td>187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292.0</th>\n",
       "      <td>218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293.0</th>\n",
       "      <td>248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294.0</th>\n",
       "      <td>221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295.0</th>\n",
       "      <td>156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296.0</th>\n",
       "      <td>139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297.0</th>\n",
       "      <td>121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298.0</th>\n",
       "      <td>129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299.0</th>\n",
       "      <td>124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300.0</th>\n",
       "      <td>131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301.0</th>\n",
       "      <td>125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302.0</th>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303.0</th>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304.0</th>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       count_pred\n",
       "249.0        4240\n",
       "250.0         647\n",
       "251.0         483\n",
       "252.0         359\n",
       "253.0         405\n",
       "254.0         233\n",
       "255.0         226\n",
       "256.0         216\n",
       "257.0         233\n",
       "258.0         236\n",
       "259.0         235\n",
       "260.0         245\n",
       "261.0         226\n",
       "262.0         262\n",
       "263.0         263\n",
       "264.0         231\n",
       "265.0         174\n",
       "266.0         230\n",
       "267.0         190\n",
       "268.0         209\n",
       "269.0         200\n",
       "270.0         185\n",
       "271.0         209\n",
       "272.0         222\n",
       "273.0         282\n",
       "274.0         213\n",
       "275.0         203\n",
       "276.0         218\n",
       "277.0         249\n",
       "278.0         223\n",
       "279.0         207\n",
       "280.0         207\n",
       "281.0         208\n",
       "282.0         201\n",
       "283.0         281\n",
       "284.0         202\n",
       "285.0         206\n",
       "286.0         192\n",
       "287.0         208\n",
       "288.0         202\n",
       "289.0         180\n",
       "290.0         212\n",
       "291.0         187\n",
       "292.0         218\n",
       "293.0         248\n",
       "294.0         221\n",
       "295.0         156\n",
       "296.0         139\n",
       "297.0         121\n",
       "298.0         129\n",
       "299.0         124\n",
       "300.0         131\n",
       "301.0         125\n",
       "302.0          95\n",
       "303.0          65\n",
       "304.0          32"
      ]
     },
     "execution_count": 489,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "id": "f7202e17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hovertemplate": "variable=count_true<br>index=%{x}<br>value=%{y}<extra></extra>",
         "legendgroup": "count_true",
         "line": {
          "color": "#636efa",
          "dash": "solid"
         },
         "marker": {
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "count_true",
         "showlegend": true,
         "type": "scattergl",
         "x": [
          70,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99,
          100,
          101,
          102,
          103,
          104,
          105,
          106,
          107,
          108,
          109,
          110,
          111,
          112,
          113,
          114,
          115,
          116,
          117,
          118,
          119,
          120,
          121,
          122,
          123,
          124,
          125,
          126,
          127,
          128,
          129,
          130,
          131,
          132,
          133,
          134,
          135,
          136,
          137,
          138,
          139,
          140,
          141,
          142,
          143,
          144,
          145,
          146,
          147,
          148,
          149,
          150,
          151,
          152,
          153,
          154,
          155,
          156,
          157,
          158,
          159,
          160,
          161,
          162,
          163,
          164,
          165,
          166,
          167,
          168,
          169,
          170,
          171,
          172,
          173,
          174,
          175,
          176,
          177,
          178,
          179,
          180,
          181,
          182,
          183,
          184,
          185,
          186,
          187,
          188,
          189,
          190,
          191,
          192,
          193,
          194,
          195,
          196,
          197,
          198,
          199,
          200,
          201,
          202,
          203,
          204,
          205,
          206,
          207,
          208,
          209,
          210,
          211,
          212,
          213,
          214,
          215,
          216,
          217,
          218,
          219,
          220,
          221,
          222,
          223,
          224,
          225,
          226,
          227,
          228,
          229,
          230,
          231,
          232,
          233,
          234,
          235,
          236,
          237,
          238,
          239,
          240,
          241,
          242,
          243,
          244,
          245,
          246,
          247,
          248,
          249,
          250,
          251,
          252,
          253,
          254,
          255,
          256,
          257,
          258,
          259,
          260,
          261,
          262,
          263,
          264,
          265,
          266,
          267,
          268,
          269,
          270,
          271,
          272,
          273,
          274,
          275,
          276,
          277,
          278,
          279,
          280,
          281,
          282,
          283,
          284,
          285,
          286,
          287,
          288,
          289,
          290,
          291,
          292,
          293,
          294,
          295,
          296,
          297,
          298,
          299,
          300,
          301,
          302,
          303,
          304,
          305,
          306,
          307,
          308,
          309,
          310,
          311,
          312,
          313,
          314,
          315,
          316,
          317,
          318,
          319,
          320,
          321,
          322,
          323,
          324,
          325,
          326,
          327,
          328,
          329,
          330,
          331,
          332,
          333,
          334,
          335,
          336,
          337,
          338,
          339,
          340,
          341,
          342,
          343,
          344,
          345,
          346,
          347,
          348,
          349,
          350,
          351,
          352,
          353,
          354,
          355,
          356,
          357,
          358,
          359,
          360,
          361,
          362,
          363,
          364,
          365,
          366,
          367,
          368,
          369,
          370,
          371,
          372,
          373,
          374,
          375,
          376,
          377,
          378,
          379,
          380,
          381,
          382,
          383,
          384,
          385,
          386,
          387,
          388,
          389,
          390,
          391,
          392,
          393,
          394,
          395,
          396,
          397,
          398,
          399,
          400,
          401,
          402,
          403,
          404,
          405,
          406,
          407,
          408,
          409,
          410,
          411,
          412,
          413,
          414,
          415,
          416,
          417,
          418,
          419,
          420,
          421,
          422,
          423,
          424,
          425,
          426,
          427,
          428,
          429,
          430,
          431,
          432,
          433,
          434,
          435,
          436,
          437,
          438,
          439,
          440,
          441,
          442,
          443,
          444,
          445,
          446,
          447,
          448,
          449,
          450,
          451,
          452,
          453,
          454,
          455,
          456,
          457,
          458,
          459,
          460,
          461,
          462,
          463,
          464,
          465,
          466,
          467,
          468,
          469,
          470,
          471,
          472,
          473,
          474,
          475,
          476,
          477,
          478,
          479,
          480,
          481,
          482,
          483,
          484,
          485,
          486,
          487,
          488,
          489,
          490,
          491,
          492,
          493,
          494,
          495,
          496,
          497,
          498,
          499,
          500,
          501,
          502,
          503,
          504,
          505,
          506,
          507,
          508,
          509,
          510,
          511,
          512,
          513,
          514,
          515,
          516,
          517,
          518,
          519,
          520,
          521,
          522,
          523,
          524,
          525,
          526,
          527,
          528,
          529,
          530,
          531,
          532,
          533,
          534,
          535,
          536,
          537,
          538,
          539,
          540,
          541,
          542,
          543,
          544,
          545,
          546,
          547,
          548,
          549,
          550,
          551,
          552,
          553,
          554,
          555,
          556,
          557,
          558,
          559,
          560,
          561,
          562,
          563,
          564,
          565,
          566,
          567,
          568,
          569,
          570,
          571,
          572,
          573,
          574,
          575,
          576,
          577,
          578,
          579,
          580,
          581,
          582,
          583,
          584,
          585,
          586,
          587,
          588,
          589,
          590,
          591,
          592,
          593,
          594,
          595,
          596,
          597,
          598,
          599
         ],
         "xaxis": "x",
         "y": [
          1,
          1,
          5,
          5,
          7,
          4,
          9,
          9,
          15,
          19,
          19,
          24,
          25,
          31,
          21,
          31,
          25,
          29,
          36,
          37,
          33,
          27,
          32,
          34,
          50,
          38,
          40,
          66,
          39,
          54,
          64,
          67,
          52,
          71,
          56,
          55,
          56,
          48,
          54,
          52,
          34,
          41,
          41,
          40,
          34,
          19,
          39,
          33,
          32,
          35,
          21,
          23,
          19,
          18,
          18,
          24,
          22,
          32,
          30,
          27,
          28,
          33,
          34,
          29,
          31,
          49,
          44,
          43,
          45,
          44,
          42,
          62,
          47,
          60,
          57,
          56,
          46,
          51,
          44,
          48,
          42,
          53,
          47,
          38,
          46,
          38,
          37,
          56,
          42,
          46,
          51,
          47,
          43,
          48,
          48,
          44,
          55,
          60,
          60,
          54,
          68,
          66,
          64,
          66,
          48,
          72,
          62,
          64,
          54,
          65,
          60,
          69,
          63,
          68,
          65,
          45,
          65,
          70,
          61,
          52,
          62,
          41,
          36,
          49,
          60,
          51,
          35,
          46,
          43,
          54,
          46,
          52,
          51,
          44,
          39,
          46,
          45,
          31,
          46,
          50,
          47,
          51,
          39,
          37,
          38,
          42,
          41,
          39,
          42,
          37,
          47,
          42,
          48,
          37,
          34,
          39,
          49,
          39,
          45,
          31,
          41,
          36,
          36,
          42,
          45,
          34,
          40,
          39,
          37,
          46,
          38,
          45,
          37,
          47,
          41,
          47,
          47,
          51,
          40,
          35,
          43,
          45,
          34,
          31,
          34,
          28,
          34,
          22,
          36,
          40,
          33,
          31,
          34,
          42,
          37,
          35,
          33,
          34,
          32,
          38,
          34,
          34,
          28,
          45,
          33,
          32,
          26,
          37,
          27,
          33,
          30,
          39,
          33,
          48,
          47,
          37,
          36,
          40,
          43,
          29,
          51,
          32,
          39,
          30,
          41,
          36,
          29,
          34,
          47,
          45,
          45,
          26,
          47,
          40,
          39,
          55,
          36,
          39,
          40,
          26,
          39,
          43,
          30,
          42,
          43,
          36,
          46,
          25,
          27,
          49,
          36,
          35,
          32,
          33,
          36,
          36,
          26,
          48,
          35,
          23,
          39,
          36,
          36,
          29,
          30,
          30,
          30,
          32,
          32,
          43,
          36,
          26,
          21,
          33,
          23,
          28,
          32,
          30,
          31,
          31,
          32,
          23,
          27,
          30,
          25,
          25,
          35,
          23,
          29,
          20,
          29,
          39,
          26,
          21,
          20,
          30,
          17,
          25,
          26,
          27,
          37,
          31,
          18,
          28,
          29,
          35,
          37,
          27,
          25,
          27,
          31,
          20,
          30,
          31,
          35,
          26,
          27,
          24,
          28,
          23,
          25,
          31,
          22,
          29,
          34,
          33,
          31,
          27,
          24,
          30,
          30,
          28,
          24,
          21,
          30,
          23,
          29,
          29,
          25,
          35,
          29,
          27,
          32,
          29,
          34,
          19,
          31,
          37,
          24,
          20,
          23,
          17,
          22,
          17,
          26,
          31,
          18,
          21,
          20,
          34,
          19,
          15,
          21,
          21,
          32,
          21,
          23,
          23,
          20,
          24,
          9,
          26,
          17,
          18,
          16,
          23,
          28,
          26,
          16,
          19,
          21,
          11,
          21,
          21,
          16,
          19,
          25,
          14,
          19,
          19,
          28,
          25,
          24,
          22,
          15,
          24,
          17,
          20,
          17,
          27,
          19,
          20,
          19,
          20,
          18,
          16,
          14,
          30,
          22,
          15,
          9,
          12,
          18,
          21,
          22,
          21,
          17,
          26,
          16,
          17,
          22,
          14,
          25,
          15,
          14,
          16,
          18,
          15,
          21,
          13,
          17,
          20,
          20,
          17,
          18,
          9,
          25,
          12,
          22,
          17,
          29,
          16,
          17,
          24,
          16,
          20,
          18,
          18,
          13,
          19,
          16,
          20,
          11,
          13,
          13,
          14,
          17,
          22,
          18,
          17,
          19,
          17,
          26,
          20,
          15,
          18,
          26,
          24,
          13,
          15,
          19,
          17,
          13,
          9,
          20,
          21,
          10,
          15,
          12,
          12,
          16,
          29,
          14,
          18,
          19,
          13,
          7,
          13,
          26,
          22,
          19,
          16,
          16,
          9,
          27,
          16,
          12,
          21,
          16,
          13,
          17,
          19,
          11,
          11,
          26,
          15,
          14,
          11,
          14,
          12,
          15,
          14,
          6,
          12,
          20,
          20,
          9,
          12,
          5,
          14,
          17,
          9,
          12,
          10,
          15,
          7,
          9,
          16
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "variable=count_pred<br>index=%{x}<br>value=%{y}<extra></extra>",
         "legendgroup": "count_pred",
         "line": {
          "color": "#EF553B",
          "dash": "solid"
         },
         "marker": {
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "count_pred",
         "showlegend": true,
         "type": "scattergl",
         "x": [
          70,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99,
          100,
          101,
          102,
          103,
          104,
          105,
          106,
          107,
          108,
          109,
          110,
          111,
          112,
          113,
          114,
          115,
          116,
          117,
          118,
          119,
          120,
          121,
          122,
          123,
          124,
          125,
          126,
          127,
          128,
          129,
          130,
          131,
          132,
          133,
          134,
          135,
          136,
          137,
          138,
          139,
          140,
          141,
          142,
          143,
          144,
          145,
          146,
          147,
          148,
          149,
          150,
          151,
          152,
          153,
          154,
          155,
          156,
          157,
          158,
          159,
          160,
          161,
          162,
          163,
          164,
          165,
          166,
          167,
          168,
          169,
          170,
          171,
          172,
          173,
          174,
          175,
          176,
          177,
          178,
          179,
          180,
          181,
          182,
          183,
          184,
          185,
          186,
          187,
          188,
          189,
          190,
          191,
          192,
          193,
          194,
          195,
          196,
          197,
          198,
          199,
          200,
          201,
          202,
          203,
          204,
          205,
          206,
          207,
          208,
          209,
          210,
          211,
          212,
          213,
          214,
          215,
          216,
          217,
          218,
          219,
          220,
          221,
          222,
          223,
          224,
          225,
          226,
          227,
          228,
          229,
          230,
          231,
          232,
          233,
          234,
          235,
          236,
          237,
          238,
          239,
          240,
          241,
          242,
          243,
          244,
          245,
          246,
          247,
          248,
          249,
          250,
          251,
          252,
          253,
          254,
          255,
          256,
          257,
          258,
          259,
          260,
          261,
          262,
          263,
          264,
          265,
          266,
          267,
          268,
          269,
          270,
          271,
          272,
          273,
          274,
          275,
          276,
          277,
          278,
          279,
          280,
          281,
          282,
          283,
          284,
          285,
          286,
          287,
          288,
          289,
          290,
          291,
          292,
          293,
          294,
          295,
          296,
          297,
          298,
          299,
          300,
          301,
          302,
          303,
          304,
          305,
          306,
          307,
          308,
          309,
          310,
          311,
          312,
          313,
          314,
          315,
          316,
          317,
          318,
          319,
          320,
          321,
          322,
          323,
          324,
          325,
          326,
          327,
          328,
          329,
          330,
          331,
          332,
          333,
          334,
          335,
          336,
          337,
          338,
          339,
          340,
          341,
          342,
          343,
          344,
          345,
          346,
          347,
          348,
          349,
          350,
          351,
          352,
          353,
          354,
          355,
          356,
          357,
          358,
          359,
          360,
          361,
          362,
          363,
          364,
          365,
          366,
          367,
          368,
          369,
          370,
          371,
          372,
          373,
          374,
          375,
          376,
          377,
          378,
          379,
          380,
          381,
          382,
          383,
          384,
          385,
          386,
          387,
          388,
          389,
          390,
          391,
          392,
          393,
          394,
          395,
          396,
          397,
          398,
          399,
          400,
          401,
          402,
          403,
          404,
          405,
          406,
          407,
          408,
          409,
          410,
          411,
          412,
          413,
          414,
          415,
          416,
          417,
          418,
          419,
          420,
          421,
          422,
          423,
          424,
          425,
          426,
          427,
          428,
          429,
          430,
          431,
          432,
          433,
          434,
          435,
          436,
          437,
          438,
          439,
          440,
          441,
          442,
          443,
          444,
          445,
          446,
          447,
          448,
          449,
          450,
          451,
          452,
          453,
          454,
          455,
          456,
          457,
          458,
          459,
          460,
          461,
          462,
          463,
          464,
          465,
          466,
          467,
          468,
          469,
          470,
          471,
          472,
          473,
          474,
          475,
          476,
          477,
          478,
          479,
          480,
          481,
          482,
          483,
          484,
          485,
          486,
          487,
          488,
          489,
          490,
          491,
          492,
          493,
          494,
          495,
          496,
          497,
          498,
          499,
          500,
          501,
          502,
          503,
          504,
          505,
          506,
          507,
          508,
          509,
          510,
          511,
          512,
          513,
          514,
          515,
          516,
          517,
          518,
          519,
          520,
          521,
          522,
          523,
          524,
          525,
          526,
          527,
          528,
          529,
          530,
          531,
          532,
          533,
          534,
          535,
          536,
          537,
          538,
          539,
          540,
          541,
          542,
          543,
          544,
          545,
          546,
          547,
          548,
          549,
          550,
          551,
          552,
          553,
          554,
          555,
          556,
          557,
          558,
          559,
          560,
          561,
          562,
          563,
          564,
          565,
          566,
          567,
          568,
          569,
          570,
          571,
          572,
          573,
          574,
          575,
          576,
          577,
          578,
          579,
          580,
          581,
          582,
          583,
          584,
          585,
          586,
          587,
          588,
          589,
          590,
          591,
          592,
          593,
          594,
          595,
          596,
          597,
          598,
          599
         ],
         "xaxis": "x",
         "y": [
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          4240,
          647,
          483,
          359,
          405,
          233,
          226,
          216,
          233,
          236,
          235,
          245,
          226,
          262,
          263,
          231,
          174,
          230,
          190,
          209,
          200,
          185,
          209,
          222,
          282,
          213,
          203,
          218,
          249,
          223,
          207,
          207,
          208,
          201,
          281,
          202,
          206,
          192,
          208,
          202,
          180,
          212,
          187,
          218,
          248,
          221,
          156,
          139,
          121,
          129,
          124,
          131,
          125,
          95,
          65,
          32,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0
         ],
         "yaxis": "y"
        }
       ],
       "layout": {
        "legend": {
         "title": {
          "text": "variable"
         },
         "tracegroupgap": 0
        },
        "margin": {
         "t": 60
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "index"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "value"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "fig = px.line(d1, x= d.index, y = ['count_true', 'count_pred'])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "id": "e0acd21e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-20T12:42:12.113791Z",
     "iopub.status.busy": "2022-02-20T12:42:12.113264Z",
     "iopub.status.idle": "2022-02-20T12:42:12.141748Z",
     "shell.execute_reply": "2022-02-20T12:42:12.141278Z",
     "shell.execute_reply.started": "2022-02-20T12:41:10.664052Z"
    },
    "papermill": {
     "duration": 0.08156,
     "end_time": "2022-02-20T12:42:12.141862",
     "exception": false,
     "start_time": "2022-02-20T12:42:12.060302",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "submission = pd.read_csv('sample_submission.csv')\n",
    "submission.num_sold = test_pred.cpu().detach().numpy().reshape(-1)\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13eec1b4",
   "metadata": {
    "papermill": {
     "duration": 0.04919,
     "end_time": "2022-02-20T12:42:12.241708",
     "exception": false,
     "start_time": "2022-02-20T12:42:12.192518",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49599ca8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 46.508647,
   "end_time": "2022-02-20T12:42:13.300393",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-02-20T12:41:26.791746",
   "version": "2.3.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "5179d32cf6ec497baf3f8a3ef987cc77c5d2dc691fdde20a56316522f61a7323"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
